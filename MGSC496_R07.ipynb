{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanwalker/MGSC496/blob/main/MGSC496_R07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Your Info\n",
        "\n",
        "your_name = '' #@param {type:\"string\"}\n",
        "your_email = '' #@param {type:\"string\"}\n",
        "today_date = '' #@param {type:\"date\"}\n"
      ],
      "metadata": {
        "id": "qCErBFYSbdOw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to \"read\" this notebook\n",
        "\n",
        "As you go through this notebook (or any notebook for this class), you will encounter new concepts and python code that implements them -- just like you would see in a textbook. Of course, in a textbook, it's easy to read code and an explanation of what it does and think that you understand it.\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "### Learn by doing\n",
        "But this notebook is different from a textbook because it allows you to not just read the code, but play with it. **You can and should try out changing the code that you see**. In fact, in many places throughout this reading notebook, you will be asked to write your own code to experiment with a concept that was just covered. This is a form of \"active reading\" and the idea behind it is that we really learn by **doing**. \n",
        "<br />\n",
        "<br />\n",
        "\n",
        "### Change everything\n",
        "But don't feel limited to only change code when I prompt you. This notebook is your learning environment and your playground. I encourage you to try changing and running all the code throughout the notebook and even to **add your own notes and new code blocks**. Adding comments to code to explain what you are testing, experimenting with or trying to do is really helpful to understand what you were thinking when you revisit it later. \n",
        "<br />\n",
        "<br />\n",
        "### Make this notebook your own\n",
        "Make this notebook your own. Write your questions and thoughts. At the end of every reading notebook, I will ask the same set of questions to try to elicit your questions, reaction and feedback. When we review the reading notebook in class, I encourage you to   \n",
        "\n"
      ],
      "metadata": {
        "id": "LHv80meh7uAs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsClHywMkHe0"
      },
      "source": [
        "# Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65kFoHxVkLIg"
      },
      "source": [
        "This reading focuses on the theory of neural networks. As such, we will not be covering coding examples in this reading, but instead trying to understand *how* neural networks are designed and how they are motivated by real neurons in the brain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AtRyQECkTRb"
      },
      "source": [
        "# From Real Neurons to Artificial Neurons\n",
        "\n",
        "Artificial neural networks were of course inspired by how the real neurons inside of our brains work.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=15D4uFFIloOQE0oYU6Zt8wYrN3VxSn12-\" width=5000>\n",
        "\n",
        "Left:\n",
        "* a picture of an actual neuron in our brain. I won't go into the details of how this work (its mostly biology that isn't that useful for our purposes).\n",
        "\n",
        "Right:\n",
        "* a model of an artificial neuron. \n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Each neuron in our brain gets inputs from many other neurons. It only fires (activates) when these inputs combine to be greater than a certain threshold.\n",
        "\n",
        "Each input into an artificial neuron is multiplied by some weight and then these values are all summed together. If the resultant sum is greater than some threshold, the neuron fires.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Mathematically, the mapping of input to output of this artifical neuron is:\n",
        "\n",
        "$\\hspace{5cm}y = \\delta(\\sum_i{w_i x_i} + b)$\n",
        "\n",
        "where:\n",
        "- $w_i$ are the weights\n",
        "- $b$ is the bias\n",
        "- $\\delta()$ is a step function (=1 if its argument is > 0; =0 otherwise)\n",
        "- note: $\\sum_i{w_i x_i}$ can be thought of as a vector dot product, $\\vec{w}\\cdot\\vec{x}$\n",
        "\n",
        "<br>\n",
        "\n",
        "We chose a step function ($\\delta$) as the **activation function**. Activation functions transform the weighted inputs of a neuron to determine whether it activates. However, there are other choices we could have made (e.g., sigmoid, RELU -- we'll talk more about these later).\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Just like a real neuron, an artificial neuron is *adaptive* -- it can be trained by adjusting its parameters (the weights and biases).\n",
        "\n",
        "<br>\n",
        "\n",
        "The example shown above is for a single neuron. It is termed the **single-layer perceptron**.  However, you can imagine that is is possible to chain a bunch of such neurons together to create a network. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\">Exercise:  Make a single-layer NN to predict whether an animal is a cat</font>\n",
        "\n",
        "Let's take some data on different animals and build our own single-layer neural network (one neuron) that predicts whether an animal is a cat based on some features. We haven't talked about how to **train** a neural network yet, so we'll just make our weights random and keep trying until we get something that gives the right prediction.\n",
        "\n",
        "Here is some data on different animals:"
      ],
      "metadata": {
        "id": "IpDtp1rvHdNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Qsh-jBDT5Wc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df = pd.DataFrame({'is_mammal':[1,1,1,0],\n",
        "                       'four_legged':[0,1,1,1],\n",
        "                       'body_weight_lbs':[70.0,8800.0,9.0,0.5],\n",
        "                       'body_height_inches':[36.0,324.0,9.0,1.0],\n",
        "                       'has_thumbs':[1,0,0,0],\n",
        "                       'animal':['chimp','elephant','cat','lizard'],\n",
        "                       'is_cat':[0,0,1,0]})\n",
        "cat_df.head()"
      ],
      "metadata": {
        "id": "D6Ft0nGD5YmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the first five columns as our features $x$. Our goal is to randomly pick five weights $w_i$ ($i=0,...,4$) and one bias $b$ and then calculate $y = \\delta(\\sum_i{w_i x_i} + b)$. \n",
        "\n",
        "<br />\n",
        "\n",
        "If $x$ is a numpy array of all the features, then we can use `np.dot(x,w)` to do the summation term, $\\sum_i{w_i x_i}$ , we can add the bias $b$ to this and then use a version of the step function $\\delta()$:.\n",
        "\n",
        "```python\n",
        "yhat = delta(np.dot(x,w)+b)\n",
        "```\n",
        "\n",
        "Before we do this, however, we should think about the weights and the features. As we learned from studying sklearn, ML models often struggle when the numerical features of data are distributed very differently. Our data includes dummy variables (which are either 0 or 1) and some numerical variables (like `body_weight_lbs` and `body_height_inches`) which have very different scales. If we're going to pick some random values for the weights and biases, we'll have to us a function from the numpy module to give us those random numbers.\n",
        "\n",
        "But how should these weights be distributed? If all of our features had mean and standard deviation close to 1, then we could draw our weights and bias from a random normal distribution. Let's do that.\n",
        "\n",
        "In a single code block:\n",
        "\n",
        "1. Use the `StandardScaler` that you learned about from studying sklearn to standardize all of the features.\n",
        "\n",
        "2. Use `np.random.rand()` to generate 5 weights and 1 bias from the random normal distribution\n",
        "\n",
        "3. Use `yhat = delta(np.dot(x,w)+b)` (I've defined the delta function defined for you below) to get a prediction(`True` or `False` for whether each data point is a cat.\n",
        "\n",
        "4. Rerun your code and watch how different random choices of weights and bias lead to different answers\n"
      ],
      "metadata": {
        "id": "6EiCa99yISSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this first\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def delta(x, thresh=0.5):\n",
        "  sigmoid = 1/(1+np.exp(-x))\n",
        "  return (sigmoid>thresh)"
      ],
      "metadata": {
        "id": "wxcqErdqXALE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Use the StandardScaler from sklearn. Fit this to the first 5 columns of cat_df and then use .transform() to transform the first 5 columns of the data (the features)\n",
        "#    and define this variable as x\n",
        "\n",
        "\n",
        "# 2. Use np.random.randn() to generate a random numpy array of 5 normally distributed weights called w. Also use it to generate a random numpy array of 1 normally\n",
        "#    distributed bias called b. Print out w and b\n",
        "\n",
        "\n",
        "# 3. Use yhat = delta(np.dot(x,w)+b) to get a prediction (for each row of the data) of whether the row is a cat and print out yhat\n",
        "\n",
        "\n",
        "# Re-run this cell multiple times and look for the result [False False True False] -- this is predicting the cat is a cat and all others are not\n",
        "# It could take a dozen or so runs before you get this desired output."
      ],
      "metadata": {
        "id": "mRuoAqZBHdN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have found a set of weights and bias that give the right prediction on the training data. Of course, we want a **training procedure** that intelligently updates the weights (rather than just pick them randomly) to minimize some error (or \"Loss Function\"). We also would want our neural network (here just a single neuron) to predict correctly on unseen data (test data). We will talk about this, but first let's talk about moving from a single neuron to a **neural network**."
      ],
      "metadata": {
        "id": "kBHAAdo1zTSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "zVeI-dkMHdN7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-ptC9O5qUGw"
      },
      "source": [
        "# A neural network\n",
        "\n",
        "Typically, we think of a neural network as a set of layers between the input and output:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=14nhiGsQVECw_XjVm7-WBftnntfYzvv0D\">\n",
        "\n",
        "The *width* of a layer is the number of neurons it contains. The *depth* of a neural network is the number of layers it contains.\n",
        "\n",
        "So, the \"deep learning\" you have likely heard about refers to building and training neural networks that have many layers.\n",
        "\n",
        "Two aspects of a Neural Network are:\n",
        "- The neural network **architecture**: This describes the connections between neurons in each layer. Above, I showed you a neural net with several \"fully connected\" layers. However there are various choices for architecture that we will talk about.\n",
        "- The neural network **training procedure**: This describes the procedure that we use to train a neural network.\n",
        "\n",
        "\n",
        "Each nueron (each circle or node in the example diagram above) has a particular bias associated with it and associates a different weight for each of its inputs (the edges in the diagram above):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=12RwFK6JFws0f9cOoK0d_aIGW_CrGKsmu\" width=500>\n",
        "\n",
        "(notice our neuron model now explicitly has the bias incorporated and a generalized activation function)\n",
        "\n",
        "\n",
        "As you can imagine, the number of parameters in a deep neural network can be quite large. How do we go about finding the right values of these parameters?  By \"right values of the parameters\", I mean the values that produce the output we want given the input.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQE_56BUrxk0"
      },
      "source": [
        "# Training a neural network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk6rppi8sxEO"
      },
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?id=10-kg9Mb-04hSeTUUW_bNQKSnDr7FMKIs\">\n",
        "\n",
        "Every neural network is essentially an approximation of a function. We are trying to approximate the function that, when it operates on the inputs will return the targets.  However, because it is an approximation, there will always be some *error*.  We term this error the \"loss\". The error depends on the parameters of the network (the weights and biases) -- and remember that there are usually a lot of parameters.\n",
        "\n",
        "Training a neural network is an optimization problem. We are seeking a \"global minimum\" - i.e., the values of the parameters that minimize the loss.\n",
        "\n",
        "Typically, when training, we will feed all of the training data through the neural network multiple times. Each time we feed all of the data through, we call that an **epoch** (epoch = one pass over the data). After each epoch, we will have a sense of which weights/biases are too big (and need to be reduced) and which are too large (and need to be increased). We will adjust the weights/biases a tiny bit in the right direction (we call this direction the **gradient**). After  many passes, we should arrive at the set of weights/biases that makes the predictions the most accurate (and therefore minimizes the loss).\n",
        "\n",
        "Training is accomplished by implementing a **training loop**, which *loops over epochs* and does the following loop in <font color=blue>pseudo-code</font>:\n",
        "\n",
        "```python\n",
        "for epoch in epochs:\n",
        "  predictions = net.forward(inputs) # 1. feedforward\n",
        "  loss = loss_function(predictions, targets) # 2. calculate loss\n",
        "  loss.backward() # 3. Backpropagate\n",
        "  optimizer.step(net.parameters) # update parameters (weights, biases)\n",
        "  zero_gradients() # set the gradients to zero\n",
        "```\n",
        "\n",
        "1. **Feedforward** - pass the input data into the network to determine the predictions (the response of the network to the inputs).\n",
        " - This is accomplished by calling the `model(inputs)` or by calling `net.forward(inputs)` depending on how the network has been defined (more on this later)\n",
        "2. **Calculate loss** - Calculate the loss by comparing the predictions to the targets\n",
        " - This is accomplished by calling `loss_function(predictions,targets)` where `loss_function()` is a loss function (there are many to choose from, more on this later) \n",
        "3. **Backpropagate** the loss - calculate the gradients of the parameters (biases and weights) using `loss.backward()`. In other words, learn which weights need to be changed the most and in which \"direction\" (increase or decrease).\n",
        "4. **Update the parameters** - Update the weights and biases to reduce the loss. In other words, change the weights so they take a little step in the right direction. \n",
        " - This is accomplished through an update function or (more commonly) through a built in optimizer, `optimizer.step()` where `optimizer` is an optimizer object (there are many to choose from, more on this later)\n",
        " - We have to be careful to zero out the gradients after each epoch (each pass through the loop), because gradients accumulate.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just introduced a lot of new terms:\n",
        "* epoch\n",
        "* loss\n",
        "* gradient\n",
        "* backpropagation\n",
        "\n",
        "Don't worry if you don't yet understand these ideas. We will talk more about them later. The important thing is for you to get a general idea of how a neural net is trained. "
      ],
      "metadata": {
        "id": "QpcNr1rv-dio"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJCOSZWlBXrZ"
      },
      "source": [
        "# Videos \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAsadkGtBfD4"
      },
      "source": [
        "\n",
        "At this point, we have enough of an idea about what neural networks are and how we train them, that watching a couple of very well-animated movies would be a perfect breather.\n",
        "\n",
        "Get your popcorn out!\n",
        "\n",
        "Run the code below to create embedded videos and watch them. You may want to read the questions below each video **before** you watch it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgsVQGoEDPqJ"
      },
      "source": [
        "# 3Blue1Brown \"But what is a Neural Network?\"\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('aircAruvnKk',width=900,height=700)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\">In Your Own Words: What Are Neural Networks</font>\n",
        "\n",
        "It's always a good idea when you read or watch something to take a moment and try to summarize it in your own words. This can help us note and remember important points and is a form of \"active listening\". Here are some targeted questions to help you do that.\n",
        "<br />\n",
        "\n",
        "In your own words:\n"
      ],
      "metadata": {
        "id": "cuaXF5IC1Yrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Imagine you are talking to a grandparent. In your own words, describe what a neural network is.\n"
      ],
      "metadata": {
        "id": "l6WVAywPE7ES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "uQevCe-VE9V2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. Explain why neural networks are made up of layers. How does this relate to breaking a problem or pattern down into parts?\n"
      ],
      "metadata": {
        "id": "p6S-5etrE8Zg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "v-lJQPZLFA66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. What is the \"Sigmoid squishification function\"? What does it do?"
      ],
      "metadata": {
        "id": "0UUGNAIdFAEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "l8c6aAlAFEr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "PgNbgx-R1Yr0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfIqvmEfFIJH"
      },
      "source": [
        "# 3Blue1Brown \"Gradient Descent, how neural networks learn\"\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('IHZwWFHWa-w',width=900,height=700)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "adH03kZdIEP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\">In Your Own Words: Gradient Descent in Neural Networks</font>\n",
        "\n",
        "It's always a good idea when you read or watch something to take a moment and try to summarize it in your own words. This can help us note and remember important points and is a form of \"active listening\". Here are some targeted questions to help you do that.\n",
        "<br />\n",
        "\n",
        "In your own words:\n"
      ],
      "metadata": {
        "id": "FXewSvspIEkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is a cost (also called a loss) function and what does it capture?"
      ],
      "metadata": {
        "id": "o0sQh-MwIEkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "x3oqBhQ4IEkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. What is a gradient and how does it relate to how we should change the weights/biases?\n"
      ],
      "metadata": {
        "id": "K6IRZqdhIEkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "IZwFqqzYIEkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. Real neurons in the brain either fire or don't fire. Why do we make artificial neurons in neural networks output a number between 0 and 1?"
      ],
      "metadata": {
        "id": "_eEFVRQgIEkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "vucSIE7IIEkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "TAWrtOsqIEkI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwFu34ApFVM-"
      },
      "source": [
        "# 3Blue1Brown \"What is backpropagation really doing?\"\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('Ilg3gGewQ5U',width=900,height=700)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\">In Your Own Words: Backpropagation in Neural Networks</font>\n",
        "\n",
        "It's always a good idea when you read or watch something to take a moment and try to summarize it in your own words. This can help us note and remember important points and is a form of \"active listening\". Here are some targeted questions to help you do that.\n",
        "<br />\n",
        "\n",
        "In your own words:\n"
      ],
      "metadata": {
        "id": "HKh-hWD9Kp40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. In as few words as possible: what is backpropagation? "
      ],
      "metadata": {
        "id": "A1Z59vppKp40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "3SjpqP02Kp40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. In a neural network, given a specific training example, suppose we focus in on one of the neurons. We'll call it the focal neuron. We can change the output of the focal neuron by changing it's weights (how it weighs its inputs) or it's bias. Assuming there is at least one or more layers of neurons *before* the focal neuron, how else could we change it's output?  (Hint: This relates to the the idea of backpropagation)\n",
        "  "
      ],
      "metadata": {
        "id": "XX3EH4lJKp40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "z7SCGbYOKp40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. What is the difference between gradient descent and stochastic gradient descent?"
      ],
      "metadata": {
        "id": "hT4WozfcKp40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "JkhIyEKRKp40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "a0BCEoREKp40"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSTTqobvFgC-"
      },
      "source": [
        "# 3Blue1Brown \"Backpropagation Calculus\" -- ONLY FOR THE BRAVE-HEARTED! You don't have to watch this one, but if you're interested, give it a shot\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('tIeHLnjs5U8',width=900,height=700)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skwmtAb68zMn"
      },
      "source": [
        "# Different Architectures?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNvcz4HZ8zle"
      },
      "source": [
        "As we will soon see, there are many different types of neuron layers and ways to connect them to one another. The situation is not as simple as a bunch of fully connected layers in between an input and output layer. \n",
        "\n",
        "Different layers include:\n",
        "- pooling layers - perform some aggregate operation on a group of neurons from the last layer.\n",
        "- convolution layers - apply some operation to a \"sliding window\" over a group of neurons from the last layer.\n",
        "- normalization layers - adjust the outputs from a past layer so they don't get \"out of control\". e.g., adjust the outputs by subtracting the mean and dividing by the standard deviation.\n",
        "\n",
        "Beyond layers, there are also completely different architectures. For example, <font color=blue>recursive neural networks (RNNs)</font> which are ideal for working with sequence data, consume each element of the sequence as an input and then produce an output that also feeds back into itself as another input (hence the term recursive):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Q3VuMWj_llhpxPwXDyas2uQ3pAmQRBrO\" width=500>\n",
        "\n",
        "In the picture above, there is **only a single RNN cell** (it is repeated to illustrate how it is used with a sequence of input data). We feed it input 1, it outputs a hidden state (and possibly output 1) that we feed back into itself along with input 2.  This repeats until we reach the end of the sequence, at which point we look at the output.\n",
        "\n",
        "We'll talk more about architectures later, both in terms of individual layers and what they do and entire network architectures that are designed for a specific purpose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5ctyKNt88UY"
      },
      "source": [
        "# Which Activation Function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJT_eyWmki6q"
      },
      "source": [
        "There are a variety of different activation functions that can be used:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBabsccpk5Fw"
      },
      "source": [
        "### Sigmoid (Logistic) and Hyperbolic Tangent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVtojaYIk8vm"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 20, 'figure.figsize': (10, 5)}) # set font and plot size\n",
        "x=np.arange(-10.,11,0.01)\n",
        "y1=(lambda x: 1/(1+np.exp(-x))) (x)\n",
        "y2=(lambda x: np.tanh(x)) (x)\n",
        "plt.plot(x,y1,'b');\n",
        "plt.plot(x,y2,'g');\n",
        "plt.grid();\n",
        "plt.xlabel('x');\n",
        "plt.ylabel('f(x)');\n",
        "plt.text(-9,0.5,'Sigmoid\\n'+r'$f(x) = \\frac{1}{1+e^{-x}}$',color=\"b\");\n",
        "plt.text(-9,-0.5,'Hyperbolic Tangent\\n'+r'$f(x) = tanh(x)$',color=\"g\");\n",
        "plt.plot(x,np.zeros(x.shape),'r:');\n",
        "plt.plot(x,np.ones(x.shape),'r:');\n",
        "plt.plot(x,-1*np.ones(x.shape),'r:');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k_6KRFIk7Yp"
      },
      "source": [
        "Both the sigmoid and the hyperbolic tangent are the most traditional activiation function. They are defined as:\n",
        "\n",
        "$f(x) = \\frac{1}{1+e^{-x}}$ (sigmoid)\n",
        "\n",
        "$f(x) = tanh(x) = \\frac{2}{1+e^{-2x}}-1$ (hyperbolic tangent)\n",
        "\n",
        "Sigmoid ranges between 0 and 1 and is a \"softer\" version of the step function. While hyperbolic tangent ranges between -1 and 1 and is a bit steeper than the sigmoid (which can lead to faster training).\n",
        "\n",
        "However, in very deep neural nets (or recurring neural nets, which we'll talk about later), its use can lead to the ***vanishing gradient problem***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpXWoYxykojF"
      },
      "source": [
        "### ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZb9CrnypIkF"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 20, 'figure.figsize': (10, 5)}) # set font and plot size\n",
        "x=np.arange(-0.1,0.2,0.01)\n",
        "y=(lambda x: np.maximum(0, x)) (x)\n",
        "plt.plot( x,y,'b-');\n",
        "plt.grid()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.text(-0.07,0.13,'ReLU\\n'+r'$f(x) = max( 0, x)$',color=\"b\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSZqFO5PkoWg"
      },
      "source": [
        "ReLU (Which stand for rectified linear unit) is a popular choice recently. It is defined as:\n",
        "\n",
        "$f(x) = max(0,x)$\n",
        "\n",
        "In other words it is equal to the input when the input is positive, but zero when the input is negative.  ReLU reduces the vanishing gradient problem significantly without any added expense that other solutions have. For this reason, it has become popular for training deep or recurrent neural nets. \n",
        "\n",
        "However it can suffer from the \"Dead Neuron\" or \"Dying ReLU\" problem: When learning rates are high, a substantial fraction of the network can be dead -- i.e., neurons that never activate once across the entire training dataset. This can happen when a neuron learns a large negative bias -- not only will the output of ReLU be zero, but also the gradient will be zero. At this point the neuron has irreversibly become dead (there's no way to update the bias).  \n",
        "\n",
        "ReLUs are often not used for the final output layer, since we typically want an output that is bounded or has a nice intrepretation (e.g., a probabilistic one)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko5acyZukiyh"
      },
      "source": [
        "### Leaky ReLU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2cGU_DFkjcC"
      },
      "source": [
        "The leaky ReLU was designed as a way to fix the \"Dying ReLU\" problem. The idea is to prevent the gradient of the activation function from becoming zero for any value of input.  \n",
        "\n",
        "$ f(x) = max(0.01x,x)$\n",
        "\n",
        "I won't show the plot, because it looks exactly like ReLU to the naked eye.  Some variants use 0.1x instead of 0.01x . Either way the \"leakiness\" is constant and just give some chance for a dead neuron to recover (since the gradient is constant and small even for large negative values of input)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utMxrY2dkgJM"
      },
      "source": [
        "### Parameterized Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSBgnDgvka6v"
      },
      "source": [
        "\n",
        "\n",
        "There are several other activation functions that can actually take parameters that can be learned through the training process. They are motivated by the two problems I mentioned above (vanishing gradient, dead neuron). And they do address these problems a bit better, however they come the cost of significantly increasing the parameters in the NN. I'm not going to talk about these in any detail, except to just mention two of them:\n",
        "\n",
        "- Parametrized ReLU: $f(x) = max(ax, x)$  where $a$ is a learned parameter\n",
        "- Exponential Linear Unit (ELU): $f(x) = max( a(e^{x}-1) , x)$ where $a$ is a learned parameter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3yY-4L3uCIO"
      },
      "source": [
        "## Last Layer Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9waA9rNMuFi4"
      },
      "source": [
        "So far we have talked about activation functions that transform the output of a single neuron. However, there are also activiation functions that transform the output of a group of neurons.\n",
        "\n",
        "It is very common to add a final layer to neural nets that transforms the output to something that can be interpreted as a probability. For example, if the final output is of size N, (e.g., in a multi-class classification network) then we would want every one of the N numbers to be between (0,1) and we would also want their sum to equal 1. \n",
        "\n",
        "<font color=blue>Softmax</font> is an activation layer that accomplishes this, by doing the following transformation:\n",
        "$S_i(\\vec{x}) = \\frac{exp(x_i)}{\\sum_k{exp(x_k)}}$ \n",
        "\n",
        "where $S_i$ is the $i^{th}$ output and $\\vec{x}$ is the input vector. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLm5csPAAxbC"
      },
      "source": [
        "# Which Loss Function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSpzxgfOAyIK"
      },
      "source": [
        "The loss function you used is very important and it depends on the type of problem you want your neural network to solve.  This could be:\n",
        "- Functional Approximation e.g., regression \n",
        "- Binary classification e.g., is this image a dog or not?\n",
        "- Multi classification e.g., which genre does this movie belong to?\n",
        "- Autoencoding e.g., get a vector representation of some text\n",
        "\n",
        "In what follows, I will show you some loss functions in terms of predicted value $p$ and target value $t$. But of course, the loss must be summed over all predictions/targets: \n",
        "\n",
        "## Mean Absolute Error Loss\n",
        "\n",
        "$Loss(p,t) = | p - t| $\n",
        "\n",
        "This is simply the absolute magnitude of the difference between the predicted and target values. This is also called the \"L1 norm\". This is useful for very simple models and linear regression, though it is not often used even in these cases, because Mean Squared Error also captures this difference and is more traditional (and thus, more easily compared to e.g., actual linear regression). This is not very useful for the more complex networks that are used in most machine learning applications. \n",
        "\n",
        "## Mean Squared Error Loss\n",
        "$Loss(p,t) = (p -t)^2$\n",
        "\n",
        "This measure the square of the difference between the prediction and target, and you are probably already familiar with it. Unlike the absolute error, the MSE loss penalizes big mistakes more (because of the square). This is also called the \"L2 norm\".  It's useful for regression and problems where numerical values are not very large and where the number of dimensions are not very large. \n",
        "\n",
        "## Smooth L1 Loss\n",
        "$Loss(p,t) = \\begin{cases} \n",
        "\\frac{1}{2}*(p - t)^2, &\\text{if}\\ |p -t| <1\\\\ \n",
        "|p - t| - \\frac{1}{2},&\\text{otherwise} \\end{cases}\n",
        "$\n",
        "\n",
        "This is not as sensitive to outliers as the MSE (because it switches to absolute value when the difference between prediction and target is high). Thus it is more suitable when values can be large (and this differences between target and prediction can be large). It can be used for many problems.\n",
        "\n",
        "## Cross Entropy / Negative Log Likelihood Loss\n",
        "For classification problems, one often designs neural networks to output the probability (as $p$) of an input belonging to each of the possible classes. The target in this case will be a \"one-hot vector\" i.e., a vector with all zeroes except for the position of the correct class. The Negative Log Likelihood Loss is then given by:\n",
        "\n",
        "$Loss(p,t) = -log(\\vec{p}\\cdot\\vec{t})$\n",
        "\n",
        "Notice that because probabilities fall between (0,1) the log will be negative, hence the negative sign cancels this negative to make the loss positive for all cases. This penalizes a model that assigns low probabilities to the correct class. Wrong predictions do not explicitly get penalized. This is useful for classification problems. If you are familiar with entropy and cross-entropy, you will know that minimizing the cross-entropy is the same as minimizing the Negative Log Likelihood. If not, you can read more about it and the related KL-divergence [here](https://machinelearningmastery.com/cross-entropy-for-machine-learning/). This is useful for binary or multi-class prediction problems.\n",
        "\n",
        "\n",
        "These are just to give you a general idea. There are of course many other loss functions that often depend on what you are trying to accomplish with your neural network.  And you may even choose to construct your own loss function. For example, you may want build a neural network that tries to predict whether two sentences came from the same book. You might have a term in your loss function that punishes the NN for making the wrong prediction proportional to how different the actual sentences are by some metric. Whatever the case, the loss function is responsible for ensuring that your network does what you want it to -- and is thus a critical choice in the design of a neural net.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6pdfp061k9E"
      },
      "source": [
        "# Which Optimizer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4H8zQTu1sr2"
      },
      "source": [
        "Perhaps the most well known optimizer is <font color=blue>Stochastic Gradient Descent</font> or other variants of Gradient Descent. Another popular choice is <font color=blue>Adam</font> (Adaptive Moment Estimatation) which you can read more about [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/). \n",
        "\n",
        "It is important to choose the right optimizer because it will determine how long it takes to train your neural network to achieve good performance. This will also depend on how much training data you have. \n",
        "\n",
        "All optimizer are trying to determine the best direction in the parameter space to move in the next step, to travel towards <font color=blue>a global minimum</font>\n",
        "<br><br>\n",
        "Optimizers vary in their choice of:\n",
        "- how big the next step should be (<font color=blue>learning rate</font>)\n",
        "- whether the size of steps should be the same in all parameter directions or whether it is <font color=blue>adaptive</font> with  different learning rates for each parameter\n",
        "- whether it should have <font color=blue>momentum</font> -- i.e., tend to move in the same direction of its last move.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedback\n",
        "What did you think about this notebook? What questions do you have? Were any parts confusing? Write your thoughts in the text box below.\n",
        "\n",
        "<font size =2> note: You can double click this text box in colab to edit it.</font>\n",
        "\n",
        "PUT YOUR THOUGHTS HERE"
      ],
      "metadata": {
        "id": "JOPdib-mkdbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit\n",
        "Don't forget to submit your notebook before class! Make sure you have saved your work (**Colab Menu: File-> Save**) and then download a pure python copy (**Colab Menu: File-> Download -> Download .py**) and a python notebook copy (**Colab Menu: File-> Download -> Download .ipynb**). You will upload both of these to the assignment on the canvas page.\n"
      ],
      "metadata": {
        "id": "-Od5aIDFnSYP"
      }
    }
  ]
}