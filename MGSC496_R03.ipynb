{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LHv80meh7uAs"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanwalker/MGSC496/blob/main/MGSC496_R03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Your Info\n",
        "\n",
        "your_name = '' #@param {type:\"string\"}\n",
        "your_email = '' #@param {type:\"string\"}\n",
        "today_date = '' #@param {type:\"date\"}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qCErBFYSbdOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to \"read\" this notebook\n",
        "\n",
        "As you go through this notebook (or any notebook for this class), you will encounter new concepts and python code that implements them -- just like you would see in a textbook. Of course, in a textbook, it's easy to read code and an explanation of what it does and think that you understand it.\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "### Learn by doing\n",
        "But this notebook is different from a textbook because it allows you to not just read the code, but play with it. **You can and should try out changing the code that you see**. In fact, in many places throughout this reading notebook, you will be asked to write your own code to experiment with a concept that was just covered. This is a form of \"active reading\" and the idea behind it is that we really learn by **doing**. \n",
        "<br />\n",
        "<br />\n",
        "\n",
        "### Change everything\n",
        "But don't feel limited to only change code when I prompt you. This notebook is your learning environment and your playground. I encourage you to try changing and running all the code throughout the notebook and even to **add your own notes and new code blocks**. Adding comments to code to explain what you are testing, experimenting with or trying to do is really helpful to understand what you were thinking when you revisit it later. \n",
        "<br />\n",
        "<br />\n",
        "### Make this notebook your own\n",
        "Make this notebook your own. Write your questions and thoughts. At the end of every reading notebook, I will ask the same set of questions to try to elicit your questions, reaction and feedback. When we review the reading notebook in class, I encourage you to   \n",
        "\n"
      ],
      "metadata": {
        "id": "LHv80meh7uAs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2gUU0qq355h"
      },
      "source": [
        "# Code Preface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YacRm5Z321E"
      },
      "source": [
        "%%capture\n",
        "!pip install parsel\n",
        "!pip install scrapy\n",
        "!pip install colab-xterm\n",
        "import parsel\n",
        "from parsel import Selector\n",
        "%load_ext colabxterm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro"
      ],
      "metadata": {
        "id": "x8OmEtw-dohE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A webscraper's fundamental task is to pull data (text, images, etc.) from various parts of webpages. To do this, we will need to write code that tells our scraper how to find and extract the data. As we will see, this can include a complex chain of logic, that might sound like this: \n",
        "\n",
        ">First go to *\\<these sites\\>*, then in each site, follow the link in *\\<this particular place\\>* to get to a new page, then grab all of the text that appears in *\\<this type of structure\\>* and all of the images that appear in *\\<this other type of structure\\>*.\n",
        "\n",
        "That probably sound a little vague right now, but will become clear when we build our own scraper. \n",
        "\n",
        "Here is how we will approach it:\n",
        "\n",
        "* First, we will learn about navigating and locating different parts of webpages, by inspecting their html and using the xpath language to \"point\" our code to different regions of the page.\n",
        "* Next, we will learn about the Scrapy framework and learn how to write simple code to crawl and extract data from webpages\n",
        "* Finally, we will learn how to use Scrapy tools and how to deploy our own scrapers, even right here in a colab notebook.\n"
      ],
      "metadata": {
        "id": "oal6P26kdp1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Navigating HTML docs with XPath\n"
      ],
      "metadata": {
        "id": "wp9AmoMAqV2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=4> Inspecting HTML docs with your browser </font>**\n",
        "\n",
        "If we want to scrape the web and extract useful data from webpages, we have to peer under the hood and see the html that the browser sees and uses to render the page. We will want to leverage patterns in the structure of the html in order to identify and locate the data (text, images) that we want to extract. The first thing that we are going to learn is how to inspect the html of web pages in our browser.\n",
        "\n",
        "Open up a new tab in your web browser and go to http://quotes.toscrape.com, then open the inspector. You can do this is Chrome, Safari or Firefox by right-clicking somewhere on the page and selecting \"Inspect\" or \"Inspect element\". In chrome, it will look something like this:\n",
        "\n",
        "<img alt=\"R03_inspecting_webpage.png\" src=\"https://drive.google.com/uc?id=1jwd7vRskmHTt8horeOKeDKjh8pUVbbkF\" width=\"100%\">\n",
        "\n",
        "In the inspection window, you can browse the element tree and click the down arrow to peer further down the branch in an element. Hovering over an element will show which the region of the webpage it corresponds to in the left pane. If you right click on an element in the inspector, you can even copy a snippet of \"code\" that specifies where that element is in the hierarchy. There are several ways to do this. To copy the xpath of the element, you would right click and select Copy->Copy XPath, like this:\n",
        "\n",
        "<img alt=\"R03_copy_xpath.png\" src=\"https://drive.google.com/uc?id=1ovWb_cvNQueCj3BRdOeAyV3yN84JGqVX\" width=\"45%\">\n",
        "\n",
        "This is the xpath that is copied when I do this for the above:\n",
        ">**/<font>html/body/div/div[1]/div[1]</font>**\n",
        "\n",
        "This \"xpath\" is actually written in a (very basic) language that we can use to tell our scrapy program which part of a page to focus on. Let's talk about that now.\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "liGdGymBye2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=4> Introducing XPath</font>**\n",
        "\n",
        "How can we specify where we want our scraper to look? To do so, we will use selectors to \"select\" different parts of the page. We have two options for languages to specify our selectors, we can use [css (cascading style sheets](https://en.wikipedia.org/wiki/CSS)) selectors or [xpath](https://en.wikipedia.org/wiki/XPath) selectors. They are just different ways to specify which element/s in a page we want to focus on and extract data from. I'm going to concentrate on xpath here, but css selectors are also an option you can explore and learn about on your own. \n",
        "\n",
        "The first thing to note is that an xpath selector may select **one or more** elements on the page (we'll use the terms element and tag interchangeably). This means that when we specify an xpath, we can expect to get back a list of one or more elements of the page (or maybe even zero elements, if nothing matches the path we specified).\n",
        "\n",
        "<br />\n",
        "\n",
        "**<font size=3> XPath: Getting children with `/`</font>**\n",
        "\n",
        "Following the [example in wikipedia](https://en.wikipedia.org/wiki/XPath#:~:text=if%20read%20carefully.-,Abbreviated%20syntax,-%5Bedit%5D), if our page looked like this:\n",
        "\n",
        ">\\<a\\><br/>\n",
        "&emsp;&emsp;\\<b\\><br/>\n",
        "&emsp;&emsp;&emsp;&emsp;\\<c /\\><br/>\n",
        "&emsp;&emsp;\\</b\\><br/>\n",
        "\\</a\\><br/>\n",
        "\n",
        "The xpath for the element C would be:\n",
        "> */a/b/c*\n",
        "\n",
        "where the `/` denotes \"child of\". The xpath says: we are looking for an element `c` that is a child of element `b` that is a child of the root node element `a`.\n",
        "\n",
        "<font size=2> Note: I'm using \\<a\\>, \\<b\\>, and \\<c\\>, but in a real example, these would be html tags like \\<div\\>, \\<span\\>, \\<img\\>, \\<a\\>, etc.\n",
        "</font>\n",
        "\n",
        "Let's try this out using an html parsing library called `parsel`. It isn't installed by defauly on colab, so you'll need to run a snippet of code first that will silently install and import the `parsel` library for you. "
      ],
      "metadata": {
        "id": "rMzWVUEfTNZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to setup parsel (you may need to rerun it if your instance disconnects)\n",
        "%%capture\n",
        "!pip install parsel\n",
        "import parsel\n",
        "from parsel import Selector"
      ],
      "metadata": {
        "id": "NT81rJugNUFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***One more thing to note***: Parsel is meant to work with documents that conform to the html standard, so it silently wraps documents with `<html><body></body></html>` tags if they are not already present in the document. I'll show you that it does this in the code below. But for clarity, all future examples will have html and body tags in the document definition. We'll have to include these in our xpath, so our example will differ a tiny bit from the one above.\n",
        "\n",
        "Ok, now run the code below."
      ],
      "metadata": {
        "id": "-EHizFcDPNvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try playing with xpath selectors\n",
        "doc = \"\"\"\n",
        "<a>\n",
        "  <b>\n",
        "    <c />\n",
        "  </b>\n",
        "</a>\n",
        "\"\"\"\n",
        "print(doc)\n",
        "selector = Selector(doc)\n",
        "print(selector.get()) # this will show that parsel wraps the document with <html><body></body></html> tags ; we'll talk about .get() later\n",
        "selector.xpath('/html/body/a/b/c') # modify this code to select the a or b element"
      ],
      "metadata": {
        "id": "0ldYgBvrNiiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of `selector.xpath(...)` is a `selector list`. In this case, the list has just one element, which is a Selector object. The selector locates the `c` tag on the page. \n",
        "\n",
        "<br />\n",
        "\n",
        "**Why is the result of a xpath selector a list of xpath selector?**\n",
        "\n",
        "A: It's a list because there may be more than one match to our xpath. Each match is a selector so that we can keep applying xpath rules to it, if we wanted to. In the example above, we could continue doing xpath operations on the selector for `c`. Though in this case, `c` doesn't enclose anything and has no attributes, so there isn't much more we could do. \n",
        "\n",
        "<br />\n",
        "\n",
        "**What's the deal with the `data=...` printed out by the selector?**\n",
        "\n",
        "You may have noticed that the selector has something called \"data\". What is this? For tags, the data of a selector is \"the tag and everything inside of it\". When we want the \"data\" inside a selector we can use the selector's `.get()` method:"
      ],
      "metadata": {
        "id": "NfumKvo9P01y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sel_list = selector.xpath('//a/b/c') # select any elements that match the xpath. The result will be a list of selectors\n",
        "sel = sel_list[0] # get the first element in the list, this will also be a selector\n",
        "sel.get() # get the data of the element located\n",
        "# modify the above code to see what happens when you select the a or b element. What does the \"data\" look like?"
      ],
      "metadata": {
        "id": "bs0wvBcaRXrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\"> Test Your Xpath </font>\n",
        "\n",
        "<font> Using the same document above, write code to select the `a` element and get its data.\n",
        "</font>    "
      ],
      "metadata": {
        "id": "KHVlg9L4GERS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here"
      ],
      "metadata": {
        "id": "QhIVeKftGERY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "RQ5O2fQKGERY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**<font size=3> XPath: Indexing Elements with `[]`</font>**\n",
        "\n",
        "Similarly, suppose the page looked liked this:\n",
        "\n",
        ">\\<html>\\<body><br/>\\<a\\><br/>\n",
        "&emsp;&emsp;\\<b\\><br/>\n",
        "&emsp;&emsp;&emsp;&emsp;\\<c /\\><br/>\n",
        "&emsp;&emsp;\\</b\\><br/>\n",
        "&emsp;&emsp;\\<b\\><br/>\n",
        "&emsp;&emsp;&emsp;&emsp;\\<d /\\><br/>\n",
        "&emsp;&emsp;\\</b\\><br/>\n",
        "\\</a\\><br/>\n",
        "\\</body>\\</html>\n",
        "\n",
        "The xpath for the `b` elements would be:\n",
        ">*/html/body/a/b*\n",
        "\n",
        "And if we wanted specifically the path to just the 2nd `b` element, the xpath would be:\n",
        ">*/html/body/a/b[2]*\n",
        "\n",
        "The ***square brackets \"index\" which elements we want that matches, starting at 1 for the first*** (note: when indexing python lists, we start at 0, but in xpath, the index starts at 1).\n",
        "\n",
        "<br />\n",
        "\n",
        "Try it out:"
      ],
      "metadata": {
        "id": "K6VppMKeM5mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"\"\"\n",
        "<html><body>\n",
        "<a>\n",
        "  <d />\n",
        "    <b>\n",
        "      <c />\n",
        "    </b>\n",
        "    <b>\n",
        "      <d />\n",
        "    </b>\n",
        "</a>\n",
        "</body></html>\n",
        "\"\"\"\n",
        "print(doc)\n",
        "selector = Selector(doc)\n",
        "selector.xpath('/html/body/a/b')"
      ],
      "metadata": {
        "id": "RuehZrMlUbjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sel_list = selector.xpath('/html/body/a/b') # this will be a \"selector list\" of two selectors. \n",
        "#Try checking the type of sel_list. What is it?"
      ],
      "metadata": {
        "id": "poMdX0A2U0Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you check the type of the above, you will notice that **this isn't a regular python list, but instead is a \"selector list\"**. This is a list-like object that behaves like a regular python list, but has some extra spice added in. Here's one cool trick: If you want the data from every item in a selector list, you can call the selector list's `.getall()` method:"
      ],
      "metadata": {
        "id": "n1gDdwUPVFNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sel_list.getall() # returns a list of the data. What is the type of this object?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuWNgvBlWJhQ",
        "outputId": "079dc437-90c7-4283-d29a-9bd5a69fad60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<b>\\n      <c></c>\\n    </b>', '<b>\\n      <d></d>\\n    </b>']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**<font size=3> XPath: Finding descendents with `//`</font>**\n",
        "\n",
        "In addition to using `/` to specify a child relationship, we can also use `//` to specify any deeper descent.\n",
        "\n",
        "Suppose the page looked like this:\n",
        "\n",
        ">\\<html\\>\\<body\\><br/>\\<d /><br/>\n",
        "\\<a\\><br/>\n",
        "&emsp;&emsp;\\<d /\\><br/>\n",
        "&emsp;&emsp;\\<b\\><br/>\n",
        "&emsp;&emsp;&emsp;&emsp;\\<c /\\><br/>\n",
        "&emsp;&emsp;\\</b\\><br/>\n",
        "&emsp;&emsp;\\<b\\><br/>\n",
        "&emsp;&emsp;&emsp;&emsp;\\<d /\\><br/>\n",
        "&emsp;&emsp;\\</b\\><br/>\n",
        "\\</a\\><br/>\n",
        "\\</body\\>\\</html\\>\n",
        "<br />\n",
        "\n",
        "If we want to match the 2nd and 3rd `d` (that are both enclosed by an `a`), but not the first `d`, we can do so with the xpath:\n",
        ">*/html/body/a//d*\n",
        "\n",
        "because both `d`'s are a descendent of an `a` (even though they aren't direct children). \n",
        "\n",
        "More importantly, `//` is even more useful because it allows us to \"start anywhere\". If we just wanted to find *any* `d` tag that was descendant of an `a` tag, regardless of whether or not that `a` tag was just under `/html/body`, we could specify it like this:\n",
        "\n",
        ">*//a/d*\n",
        "\n",
        "You will see many xpaths like this that ***do not reference the root node of the document***. \n"
      ],
      "metadata": {
        "id": "JHeTCtZoS7O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"\"\"\n",
        "<html><body>\n",
        "<d />\n",
        "<a>\n",
        "  <d />\n",
        "    <b>\n",
        "      <c />\n",
        "    </b>\n",
        "    <b>\n",
        "      <d />\n",
        "    </b>\n",
        "</a>\n",
        "</body></html>\n",
        "\"\"\"\n",
        "print(doc)\n",
        "selector = Selector(doc)\n",
        "selector.xpath('//a/d')"
      ],
      "metadata": {
        "id": "nXPpk_zUX3G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "**<font size=3> XPath: Wildcards</font>**\n",
        "\n",
        "We've been using explicit tags in all the xpath examples, so far, but it is also possible to use the wildcard `*`. Using the page from the last example, if we wanted to select every tag that was a child of an `a` element (in the above, this would be the second tag `d` and the two `b` tags), we could do so with the xpath:\n",
        ">*//a/\\**\n",
        "\n"
      ],
      "metadata": {
        "id": "r_9VdK6UX0Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selector.xpath('//a/*') # Select every tag that is a child of an a \n",
        "# Now modify the above to instead select every tag that is a descendant of a"
      ],
      "metadata": {
        "id": "oiolG2FmDAD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\"> Test Your Xpath </font>\n",
        "\n",
        "<font> Using the same document above, write a single line of code that returns a selector list of every element that is a **descendant** of an element `a`\n",
        "</font>    "
      ],
      "metadata": {
        "id": "2nbn63IYHgKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here"
      ],
      "metadata": {
        "id": "psZPiwYUFk7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "GuV5-4t4HgKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=3> XPath: Selecting tags based on attributes, using `[@attr ...]`</font>**\n",
        "\n",
        "The most useful thing that xpath will let us do is to select nodes/tags conditionally, and especially based on the value of attributes of tags. Recall that html tags have attributes, like this:\n",
        "\n",
        "`<a href=\"http://wikipedia.com\" class=\"reflink\">This is a link</a>`\n",
        "\n",
        "here the two attributes of the **a** tag are `href` and `class`. Class attributes are used widely in modern webpages and are a great \"hook\" for us to find a reference to the part of the webpage we want. For example, suppose we had a webpage which had a bunch of reference links in it, and we knew that each one had the attribute `class=\"reflink\"`, we could make an xpath selector that would yield a list of all of those links using this xpath:\n",
        ">*//a[@class=\"reflink\"]*\n",
        "\n",
        "Note that we can put all kinds of expressions inside of the square brackets. For example, suppost we had a document that had lots of images of different sizes, and we wanted to extract all images smaller than some width (maybe these are thumbnails we want to download), we might do so with an xpath like this:\n",
        "\n",
        ">*//img[@width<300]*\n",
        "\n",
        "Of course, you can also combine multiple conditions using `and` and `or` in expressions inside the brackets.\n",
        "\n",
        "<br/>\n"
      ],
      "metadata": {
        "id": "0l2n4EJkHanN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=3> XPath: Getting attributes and text within tags</font>**\n",
        "\n",
        "Sometimes you may want to extract the actual attribute of a tag using xpath. Suppose you had some html with a bunch of span tags, like this:\n",
        "\n",
        "```\n",
        "<div class=\"toyblock>\n",
        "  <span class=\"imgwrapper\"><img src=\"voltron.png\" /></span>\n",
        "  <span class=\"toyname\">Voltron</span>\n",
        "  <span class=\"price\">$14.99</span>\n",
        "</div>\n",
        "<div class=\"toyblock>\n",
        "  <span class=\"imgwrapper\"><img src=\"he-man.png\" /></span>\n",
        "  <span class=\"toyname\">He-Man</span>\n",
        "  <span class=\"price\">$5.99</span>\n",
        "</div>\n",
        "```\n",
        "and you wanted to get the `class` attribute of the span tags under a `toyblock` div, you could do it like this:\n",
        ">//div[@class=\"toyblock\"]/span/@class\n",
        "\n",
        "This would give you all of the element class attributes that were children of the `toyblock` div tag.\n",
        "\n",
        "Suppose you wanted to get the text inside the span tags for the toynames? You can do that with the by adding `text()` to the end of the xpath, like this:\n",
        ">//span[@class=\"toyname\"]/text()\n",
        "\n",
        "You can even conditionally match elements based on their inner text using `text()` inside square brackets, though this is less common because inner text in webpages may not match a common pattern.\n",
        "\n",
        "<br/>\n"
      ],
      "metadata": {
        "id": "6FV6u9yPtL-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"\"\"\n",
        "<html><body>\n",
        "<div class=\"toyblock>\n",
        "  <span class=\"imgwrapper\"><img src=\"voltron.png\" /></span>\n",
        "  <span class=\"toyname\">Voltron</span>\n",
        "  <span class=\"price\">$14.99</span>\n",
        "</div>\n",
        "<div class=\"toyblock>\n",
        "  <span class=\"imgwrapper\"><img src=\"he-man.png\" /></span>\n",
        "  <span class=\"toyname\">He-Man</span>\n",
        "  <span class=\"price\">$5.99</span>\n",
        "</div>\n",
        "</body></html>\n",
        "\"\"\"\n",
        "print(doc)\n",
        "selector = Selector(doc)\n",
        "selector.xpath('//span[@class=\"toyname\"]/text()').getall() # get a list of the inner text of span tags with class=\"toyname\""
      ],
      "metadata": {
        "id": "XQHSXU_jtZdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\"> Test Your Xpath</font>\n",
        "\n",
        "<font> Using the same document as in the above code, write a single line of code that generates a list of the image .png filenames for the toys on the page.   \n",
        "</font>    "
      ],
      "metadata": {
        "id": "EJxE9h44Ezxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here\n"
      ],
      "metadata": {
        "id": "2BPFGABIFAWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "8Nv7bk_IEzxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=3> XPath in the real world</font>**\n",
        "\n",
        "Ok, we have seen enough \"artificial\" examples. Let's move to some real world ones. We need a way to grab pages from the web. We will do this with the `requests` module. Let's use that to grab the content of the page we want to scrape, http://quotes.toscrape.com. Don't forget we can inspect this page in our browser. You should do so now to compare the results of the code below with what you see in the inspector and on the page.\n"
      ],
      "metadata": {
        "id": "vUObxVFSv4jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab the webpage and make a selector object from its html \n",
        "import requests\n",
        "page = requests.get('http://quotes.toscrape.com') # fetch the webpage \"http://quotes.toscrape.com\"\n",
        "doc = page.text # store the raw html of the page\n",
        "selector = Selector(doc) # make an xpath selector object from the raw html"
      ],
      "metadata": {
        "id": "XOxClgO6u7PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New we can do any selector we want. First, let's try to get a sense of how the designer of this webpage used classes. We'll start with the `<div>` tag, and see what classes they gave it:"
      ],
      "metadata": {
        "id": "71OHgsOziYQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what classes div tags have on the page we want to scrape\n",
        "div_classes = selector.xpath('//div/@class').getall() # grab a list of all of the div classes that occur on the page\n",
        "div_classes = list(set(div_classes)) # this trick only keeps the unique elements of the list, by converting it to a set and then back to a list\n",
        "print(div_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12WfTat5iUSt",
        "outputId": "c7be63a4-4590-466f-eeba-4ea7abfa6162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['col-md-4 tags-box', 'quote', 'tags', 'col-md-8', 'row', 'row header-box', 'col-md-4', 'container']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we notice is that some `<div>`'s have multiple classes. For example, `'col-md-4 tags-box'` is actually two classes. If we need to find elements that have multiple classes we can't just use an xpath with `[@class='...']`. The `parsel` documentation [has some suggestions about dealing with multiple classes](https://parsel.readthedocs.io/en/latest/usage.html#when-querying-by-class-consider-using-css), but for now we'll ignore these.\n",
        "\n",
        "If we want to scrape the quotes, the most relevant class seems to be `quote`, so let's check it out:"
      ],
      "metadata": {
        "id": "eV01zWX8J-Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What does the html inside one of these \"quote\" div tags look like?\n",
        "selector.xpath('//div[@class=\"quote\"]').get() # .get() applied to the selector list will give us the data of the first element of the list"
      ],
      "metadata": {
        "id": "JPfK62wRjn2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above, we can see that the actual quote seems to be enclosed in a `<span class=\"text\">` tag, so let's get the text enclosed in all of those: "
      ],
      "metadata": {
        "id": "afii-ua3k42Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quotes = selector.xpath('//div[@class=\"quote\"]/span[@class=\"text\"]/text()').getall()\n",
        "print(quotes)\n",
        "len(quotes)"
      ],
      "metadata": {
        "id": "_KrxMJ2VlIu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! Those are all the quotes on the page, now in a nice list. We would still like to get the authors and the tags for each quote."
      ],
      "metadata": {
        "id": "N1ZcK4T_lwg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\">Use xpath to get a list of the authors of all the quotes</font>\n",
        "\n",
        "<font> Using your browser inspector and python code find a way to:\n",
        "\n",
        "    * create a list of all of the authors of the quotes.\n",
        "    * create a list of all of the links to about for each author\n",
        "</font>    "
      ],
      "metadata": {
        "id": "ZDV8c-xtmKOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here\n"
      ],
      "metadata": {
        "id": "Mpb92hkrmKOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "25DcACrFmKOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you've gotten a taste for using xpath, we're ready to start talking about the Scrapy webscraping framework."
      ],
      "metadata": {
        "id": "gT5asGnFzYwi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsyIK-p_Fo0D"
      },
      "source": [
        "# Scrapy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=4>What is Scrapy?</font>**\n",
        "\n",
        "<img alt=\"R03_scrapy_logo.png\" src=\"https://drive.google.com/uc?id=1OcMETu2Q6TS9717-gG5EENhePNc7fHGx\" width=\"30%\">\n",
        "\n",
        "\n",
        "[Scrapy](https://scrapy.org/) is a framework for writing your own webscrapers. It is incredibly fast, incredibly powerful, and easily extended to suit many projects. For some small scraping tasks, it may even seem like overkill. However, once you learn how easy it is to make a scrapy crawler project with your own spider and item data definitions, you will be tempted to use it even for small tasks.  \n",
        "\n",
        "**<font size=4>Scrapy Components</font>**\n",
        "\n",
        "We will learn about several components of Scrapy, including:\n",
        "* *Spiders*: Classes that define how a site will be scraped (how to \"crawl\" or follow links, and how to extract data from pages)\n",
        "* *Selectors*: These are the same xpath selectors you just learned about.\n",
        "* *Items*: A python class that reflects the structure of the data objects you want to extract (e.g., a quote has the body of the quote, an author field, tags, etc.)\n",
        "* *Scrapy command line*: A command line tool that lets you create and launch scrapy projects and specify crawling options, such as where to store the scraped data.\n",
        "* *Scrapy shell*: An interactive python shell that can be launched to explore webpages and how your spider code interacts with them. It can also be launched when your scraper encounters an exception, to let you explore what might have gone wrong.\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "**<font size=4>Scrapy Architecture</font>**\n",
        "\n",
        "First, I'll give you the description of how Scrapy works directly from the mouths of the developers, then I'll give you my own simplified description. Don't worry if you don't understand the developer's description, ***just skim it to get an idea.*** The [developers say](https://docs.scrapy.org/en/latest/topics/architecture.html):\n",
        "\n",
        "><img alt=\"R03_scrapy_architecture.png\" src=\"https://drive.google.com/uc?id=1p0hLGUdUjfkJiVcEAmpfZ6GgebP9FXTK\" width=\"100%\">\n",
        "The data flow in Scrapy is controlled by the execution engine, and goes like this:\n",
        ">\n",
        ">1. The Engine gets the initial Requests to crawl from the Spider.\n",
        ">\n",
        ">2. The Engine schedules the Requests in the Scheduler and asks for the next Requests to crawl.\n",
        ">\n",
        ">3. The Scheduler returns the next Requests to the Engine.\n",
        ">\n",
        ">4. The Engine sends the Requests to the Downloader, passing through the Downloader Middlewares (see process_request()).\n",
        ">\n",
        ">5. Once the page finishes downloading the Downloader generates a Response (with that page) and sends it to the Engine, passing through the Downloader Middlewares (see process_response()).\n",
        ">\n",
        ">6. The Engine receives the Response from the Downloader and sends it to the Spider for processing, passing through the Spider Middleware (see process_spider_input()).\n",
        ">\n",
        ">7. The Spider processes the Response and returns scraped items and new Requests (to follow) to the Engine, passing through the Spider Middleware (see process_spider_output()).\n",
        ">\n",
        ">8. The Engine sends processed items to Item Pipelines, then send processed Requests to the Scheduler and asks for possible next Requests to crawl.\n",
        ">\n",
        ">9. The process repeats (from step 3) until there are no more requests from the Scheduler.\n",
        "\n",
        "<br />\n",
        "\n",
        "**<font size=4>Simplified Scrapy Architecture</font>**\n",
        "\n",
        "Since we will be only writing code for the Scrapy spider, we can only focus on this and ignore how the rest of the Scrapy engine works. Here's my own take on how a scrapy spider works:\n",
        "\n",
        "\n",
        "<img alt=\"R03_scrapy_architecture_dylan.png\" src=\"https://drive.google.com/uc?id=1bLeUINw85Mi_6AkJB3m-pGFmGEilx67o\" width=\"100%\">\n",
        "Requests are shown as orange lines. Responses are shown as green lines. Data items are shown as dashed blue lines.\n",
        "\n",
        "\n",
        "1. The spider begins by taking the `start_urls` one by one, making them into request objects and feeding them to the back of the line of the **request queue**\n",
        "\n",
        "2. A `request` object is taken from the front of the line. Each request specifies not only the url but also which `callback` method of the spider will be used to handle the `response`. The request is then sent out through the internet to the url's webserver.\n",
        "\n",
        "3. The url's webserver generates a `response` to the request. This response has a reference to the request that generated it. So it knows which callback method was specified. Scrapy calls the specified callback method and passes it the `response` as an argument.\n",
        "\n",
        "4. Parse methods handle responses. They may sort through the html of the response (e.g., with `.xpath()`) and can yield **data items** (things we want to save to files) and even **generate further requests** (e.g., if the page has a \"next\" link, generate a new request with the next link as its url). Any new requests that are generated will have a callback method specified to determine who will handle their response and will be fed onto the back of the line of the request queue.\n"
      ],
      "metadata": {
        "id": "HuVE7c83LuOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=4>Writing Simple Scrapy Spiders</font>**\n",
        "\n",
        "What does this mean for us practically? It means that to write a webscraper with scrapy, all we have to do is to fill in the blanks of some Spider class code by writing some parse methods. Here is what the code from a \"blank\" Scrapy Spider looks like:\n",
        "\n",
        "```python\n",
        "# MySpider.py -- a blank Scrapy Spider\n",
        "import scrapy\n",
        "\n",
        "class MySpider(scrapy.Spider):\n",
        "    name = 'myspider'\n",
        "    allowed_domains = ['some.domain.com']\n",
        "    start_urls = ['http://some.domain.com/somepage']\n",
        "\n",
        "    def parse(self, response):\n",
        "      # YOUR CODE HERE TO: parse the response, yielding data items and/or other requests\n",
        "      pass\n",
        "```\n",
        "\n",
        "In other words, we just have to write code for what the `parse()` method does. Sometimes, we might have only a single parse method. Sometimes, we will want to write more (`parse2()`, etc.) and assign them as callbacks to different types of requests. \n",
        "\n",
        "You might be wondering how we can get data out of this. The simplest way is to just pull the data out by using xpath operations, assign this to python variables, and then make a dictionary where the keys are the names and the values are the python variables we want to save. Here's ***an example for a hypothetical website that we want to pull toy names and prices from***.   \n",
        "\n",
        "```python\n",
        "# ToySpider.py -- a hypothetical spider for scraping names and prices from a toy site\n",
        "class ToySpider(scrapy.Spider):\n",
        "    name = 'toyspider'\n",
        "    allowed_domains = ['cool.toys.com']\n",
        "    start_urls = ['http://cool.toys.com/new']\n",
        "    \n",
        "    def parse(self,response):\n",
        "      sel_list = response.xpath('/xpath/to/toyitem')\n",
        "      for toy_sel in sel_list:\n",
        "        toyname = toy_sel.xpath('continuedpath/to/toyname/text()').get()\n",
        "        toyprice = toy_sel.xpath('continuedpath/to/toyprice/text()').get()\n",
        "        yield {'toyname': toyname, 'toyprice': toyprice}\n",
        "```\n",
        "\n",
        "Let's walk through what it does. First it uses xpath to locate a  'toy item' block. Then it loops over every such toy item and uses relative xpaths to find the name and price of the toy in the block. Lastly, it creates a dictionary with the toyname and toyprice and `yield`'s it. Remember, a yield is like a `return` that doesn't end the function.\n",
        "\n",
        "Of course, this spider would only be able to handle a single page (the single `start_url` we gave it). We could have added more urls to that `start_url` list, but what if we didn't know what they were? What if we just knew that there was a \"next\" button somewhere on the page and we wanted our spider to \"click it\" (i.e., follow its link and then process the page you landed on). We could add some code to do that, like this:\n",
        "\n",
        "```python\n",
        "# ToySpider.py -- a hypothetical spider for scraping names and prices from a toy site\n",
        "class ToySpider(scrapy.Spider):\n",
        "    name = 'toyspider'\n",
        "    allowed_domains = ['cool.toys.com']\n",
        "    start_urls = ['http://cool.toys.com/new']\n",
        "    \n",
        "    def parse(self,response):\n",
        "      sel_list = response.xpath('/xpath/to/toyitem')\n",
        "      for toy_sel in sel_list:\n",
        "        toyname = toy_sel.xpath('continuedpath/to/toyname/text()').get()\n",
        "        toyprice = toy_sel.xpath('continuedpath/to/toyprice/text()').get()\n",
        "        yield {'toyname': toyname, 'toyprice': toyprice}\n",
        "      # Here is some code to find the next link and yield a request for it:\n",
        "      next_relative_url = response.xpath('/xpath/to/next/link/@href').get() \n",
        "      next_url = response.urljoin(next_relative_url) \n",
        "      yield scrapy.Request(url=next_url,callback=self.parse)\n",
        "```\n",
        "\n",
        "In the above, to follow the next link, we had to extract the url of the \"next\" link from the response html, and then conver it from a relative url to an absolute one (e.g., `/page/2` is relative in that you assume it has the same domain as the page the link is on, whereas an absolute url would be `quotes.toscrape.com/page/2` ) and then yield a new request. In fact, since following a link from the present page is such a common practice, the scrapy response object has a method which combines these last two lines of code into one:\n",
        "```python\n",
        "      yield response.follow(url=next_url, callback=self.parse)\n",
        "```\n",
        "\n",
        "In both of the hypothetical examples shown above, we used only a single method `parse` to handle responses, but we could have defined any number of methods with whatever method names we wanted. \n",
        "\n",
        "Now that we have the basic idea of how to write a scraper, let's looks at how we can use and deploy scrapy."
      ],
      "metadata": {
        "id": "4XDsZwRFTN3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=4>Using Scrapy in Google Colab</font>**\n",
        "\n",
        "Scrapy projects are made up of several (object oriented) pieces of python code (`.py` files) that work together. As is common with OOPs (objected oriented projects), class definitions are usually stored in their own `.py` python files. Typically, such projects are wrapped up in a module structure, so that they can be distributed this way and imported by others. There is usually an entrypoint for such projects -- some particular `.py` file that you run to start everything running. Scrapy has its own command line tool for creating and launching projects. For example, to start a scrapy crawler for which you have already written the code, you would call `scrapy crawl projectname` from within the project directory of an operating system shell/command prompt (e.g., osx terminal, windows command line, or even, as we will see, a colab xterm terminal or `!command` in a colab cell)\n",
        "\n",
        "Using Scrapy from Colab is a bit funny, because Scrapy is not designed to be run as a python notebook. But in practice, its very easy and convenient to get it to do so.\n",
        "\n",
        "<br />\n",
        "\n",
        "**<font size=3>Install Scrapy</font>**\n",
        "\n",
        "First we need to make the google machine instance that we are using with our notebook install scrapy, because it is not one of the libraries that is pre-installed.\n",
        "\n",
        "The following code will do this. You will notice that several other libraries  will be installed by this command. These are Scrapy's *dependencies*\n",
        "\n",
        "<font size=\"2\">*Pro tip: sometimes a code cell in colab will produce a lot of output that you would rather not see. If you want to silence the output of a code cell, you can just make the first line</font>* `%%capture`\n"
      ],
      "metadata": {
        "id": "aswaVOhWMCnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy"
      ],
      "metadata": {
        "id": "5Wp3I6W8LgQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Also, we will want to use the scrapy command line tools, so we will need some access to a system terminal (i.e., \"shell\" or \"command line\" access).  Unfortunately, the free version of coalb doesn't come with a terminal. If you pay for **colab pro**, there is a convenient terminal button on the side menu (the same sidebar menu that has the file explorer). As an alternative, we will install the `colab-xterm` library and load it as an extension to colab:"
      ],
      "metadata": {
        "id": "AktiwK9KiJ4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install colab-xterm\n",
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "J0Iuc1qevUPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Great, we're ready to go."
      ],
      "metadata": {
        "id": "22Q8364ui_U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**<font size=3>Scrapy Command Line: Scrapy Shell</font>**\n",
        "\n",
        "Scrapy has a command line tool that we can use to do a number of things to get our scrapy project up and running. Check out the [scrapy command line documentation](https://docs.scrapy.org/en/latest/topics/commands.html#command-line-tool). \n",
        "\n",
        "\n",
        "The first Scrapy command we'll talk about is running a scrapy shell. Scrapy shell is an interactive python environment with a default scrapy crawler invoked, giving access to many of the objects that are used by the scrapy engine. It's a good place to experiment with xpath and develop the spider code that will parse your webpage. [Remember earlier](#scrollTo=vUObxVFSv4jz) when we used the `requests` and `parsel` libraries to extract data from `quotes.toscrape.com`? Scrapy shell provides a similar environment with some extra access to the other objects that Scrapy uses. Scrapy shell can also be called from your scraper when things go wrong to help you \"debug\" it. \n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "Let's run Scrapy shell now. If we were doing this on a local installation of python, we would just open up a terminal or \"command prompt\" (e.g., terminal in osx; cmd in windows) and type the command: \n",
        "\n",
        "`scrapy shell websiteaddress`\n",
        "\n",
        "But since we are in colab, we will use a `colab-xterm` to do this. Run the below code cell to launch the terminal and inside that terminal type:\n"
      ],
      "metadata": {
        "id": "UHMynQqOPts8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "iej3ff-5h2WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the shell above, we are going to explore the html `response` received by vising the site [quotes.toscrape.com](quotes.toscrape.com). To do this **in the terminal window that opens above**, type the following command:\n",
        "```\n",
        "scrapy shell \"quotes.toscrape.com\"\n",
        "```\n",
        "Your output should look something like this:\n",
        "\n",
        "<img alt=\"R03_scrapy_shell_in_xterm.png\" src=\"https://drive.google.com/uc?id=1D9DmAlVJR6v9NHVlj4D-E6yQP2Fbt5fL\" width=\"100%\">\n",
        "\n",
        "<font color=\"darkred\" > **If you'd prefer to run the colab xterminal in a separate window so that you can keep it open and tab between it and this notebook, I have provided a notebook `MGSC496-ScrapyXterm.ipynb` with just scrapy and a terminal.** Note that colab will generally let you edit any number of notebooks at the same time, but will only yet you have one running (unless you pay for colab pro).</font>\n",
        "\n",
        "Scrapy shell tells you about the various objects that are defined in this interactive environment.These are: the `scrapy` module, and some objects such as the `crawler`, `spider`, and `settings` objects which we'll talk about later, as well as the `request` and `response` objects, which relate directly to the webpage that we told scrapy shell to fetch for us. We sent a web **request** to `quotes.toscrape.com` and we got back a **response**.  We can do this for any url from within scrapy shell by using the `fetch('some.website.com')`. \n",
        "\n",
        "If we were running Scrapy on a local machine instead of in a cloud notebook, we would be able to view the page we fetched in our system's default browser using `view(response)`. However, this won't work in colab. Instead, we can simply open the page we fetched in a browser on our local machine if we would like to look at the page and inspect the html. We will need to do this to design our scraper.\n",
        "\n",
        "Let's explore the `request` and `response` objects in scrapy shell in our terminal window."
      ],
      "metadata": {
        "id": "kaOGbpoH7OIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\"> Explore the `request` and `response` objects in Scrapy Shell </font>\n",
        "\n",
        "<font> What kinds of things are in the `request` and `response` objects? Use methods like `type`, `dir` `id` and `print` functions to explore these objects \n",
        "</font>    "
      ],
      "metadata": {
        "id": "dKxuyrBOHLG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double click this text box to edit it to add your answers to the questions below\n",
        "\n",
        "* Explain what each of these is, in your own words:\n",
        "\n",
        " * `request.url`: \n",
        " * `request.headers`:\n",
        " * `request.method`:\n",
        " * `response.url`:\n",
        " * `response.ip_address`:\n",
        " * `response.headers`:\n",
        " * `response.status`:\n",
        " * `response.body`:\n",
        " * `response.request`:\n",
        "\n",
        "* Why does `response` have a `request` object inside of it? \n",
        "\n",
        "* Is it the same as the request object or is it different? (hint: [use python's `id()` function](https://docs.python.org/3/library/functions.html#id) to answer this question).\n",
        "\n",
        "* Use `fetch()` to fetch a different page from within scrapy shell"
      ],
      "metadata": {
        "id": "tTw19UNJKKYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "p0k-ZgFgHLHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=3>A Brief Aside about HTTP</font>**\n",
        "\n",
        "*If we're going to scrape webpages, we need to know a little about the http protocol. I will keep this super brief at the risk of falling down a rabbit hole.*\n",
        "\n",
        "To get a webpage, we form a web request which includes the url of the page we want and the http method we are going to use to request it (either a `GET` or `POST`). Our request may carry some information to the webserver (that includes what kinds of things our client machine can do -- what we \"Accept\" -- what kind of language pages we accept -- \"Accept-Language\" and even what kind program is asking (e.g., a browser) -- this is contained in the \"user-agent\" header field. All of this extra information lives in `http headers`. **Have a look at `request.headers` in the shell now.**\n",
        "\n",
        "When the webserver recevies our http request, it sends back an http response message. This message also has headers. Have a look at them now in the shell. In addition, it contains a status field, with a code that indicates if there was a problem with the request/response. [Here is a list of different http codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). And, of course, the response contains the body of the page that you asked for itself (in html), under `response.body`.**Have a look at `response.body` now in in the shell.** You will see it is a lot of html that is sorta human readable but not easily. Fortunately, you have the browser inspector and xpath to help you play with it. \n",
        "\n",
        "<br />\n"
      ],
      "metadata": {
        "id": "IWvHKD966CgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=3>Scrapy Command Line: Making a new project</font>**\n",
        "\n",
        "Scrapy projects are made up of several files. But we only need to customize a little bit of code to make our scraper. We can generate a blank new project using the scrapy command `scrapy startproject projectname` like this:\n"
      ],
      "metadata": {
        "id": "fpDIfnEjGaSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy startproject quotescraper"
      ],
      "metadata": {
        "id": "y3IWiJXHGn3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can immediately see all the files this created by looking at the colab file browser:\n",
        "\n",
        "\n",
        "<img alt=\"R03_scrapy_project_files.png\" src=\"https://drive.google.com/uc?id=1g71vYBhY_R8Op1Zas1holjNiS9kHtik6\">\n",
        "\n",
        "\n",
        "You can see that a Scrapy project has:\n",
        "- `/spiders/`: A place put your spider code, which governs the behavior of one or more \"spiders\" that crawl the web\n",
        "- `settings.py`: A place to configure scrapy project settings\n",
        "- `items.py`: A place to define the data structure of stuff that we will extract with our scraper (we can define this and yield our own special item objects, instead of just yielding a dictionary)\n",
        "- We won't discuss the other two (`pipelines.py` and `middlewares.py`)\n",
        "\n",
        "For many simple scrapers, all we need to do is run this startproject command and then write code for a single spider and save it as a `.py` file in this `/spiders/` folder. We can write all the code from a cell in colab to a file using the `%%writefile` command as the first line of the cell.\n",
        "\n",
        "Run the code below:\n",
        "\n"
      ],
      "metadata": {
        "id": "4ifISIkyHkFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/test.py\n",
        "print(\"This is a test.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hxtatF8JLZJ",
        "outputId": "5d113cb9-db3c-4ae5-b469-3584757deaa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use the file browser in colab to verify that the file `/content/test.py` was created and open it to see its contents. Once you have done so, you can safely delete this file if you want. Notice that the default home directory for colab is `/content`.\n",
        "\n",
        "One annoying thing about colab is that, when the first line of the code cell is `%%writefile`, you will lose all the nice python context (code will no longer be colored to highlight syntax). Fortunately, we can comment out the first line like this `#%%writefile` to get our colors back. Once we're done working with the code in that cell, we can remove that  the `#` and run the cell to write the code to the file.\n",
        "\n",
        "We will use this trick to write our `quotespider.py` file."
      ],
      "metadata": {
        "id": "NYrqNO79JneG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=3>Scrapy Command Line: Generating stub code for a spider</font>**\n",
        "\n",
        "You don't have to write the code for your spider completely from scratch, or even copy and paste starter code from somewhere else, because scrapy command line also has a tool that will generate some boilerplate code for your spider. Just enter the directory of your project and run:\n",
        "```\n",
        "scrapy genspider spidername somedomain.com\n",
        "```\n",
        "\n",
        "Let's do this now to see the boilerplate code that it generates. Don't forget that when you want to execute a shell command in a specific location in colab, you should use `&&` chaining to change to the desired directory and then execute the command. Try running the following:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KwenpBG19PoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/quotescraper && scrapy genspider quotespider quotes.toscrape.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "111iUu599v7P",
        "outputId": "3dce4a23-6fd9-428f-e9fe-bf8cd00cd80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created spider 'quotespider' using template 'basic' in module:\n",
            "  quotescraper.spiders.quotespider\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now open the file `/content/quotescraper/quotescraper/spiders/quotespider.py` by finding it in the colab file explorer and double clicking it.\n",
        "\n",
        "This is helpful if its been a while since you've used scrapy and you want a quick start. When we work on scrapy projects in colab, we will use the `%%writefile` trick discussed above to write our spider code to a `.py` file in the correct location, so we won't use genspider. If you want to use it, you can generate the spider code and then copy and paste it to a cell."
      ],
      "metadata": {
        "id": "XIsOvLgT-pfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size=3>Scrapy Command Line: Running a crawler project</font>**\n",
        "\n",
        "Once we've written our code for the spider, we can run our project from the command line by entering the top level directory of the project and running the command `scrapy crawl projectname`. Typically, we will want to save the output of a scraper to a file. We can specify this with the `-o` or `-O` option:\n",
        "```\n",
        "scrapy crawl projectname -o outputfilename.json\n",
        "```\n",
        "The above will output data to the json file (default format). Using `-o` will append data to the end of the file if it exists already, whereas `-O` will overwrite the file. To see other options, check out  the [documentation for the scrapy crawl command](https://docs.scrapy.org/en/latest/topics/commands.html#crawl).\n",
        "\n",
        "\n",
        "Remember that scrapy commands should always be run in your project folder, so we would use:\n",
        "```\n",
        "!cd /content/quotescraper && scrapy crawl quotescraper -o output.json\n",
        "```"
      ],
      "metadata": {
        "id": "NeCmgkBd3hCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\"> Your First Scrapy Scraper</font>\n",
        "\n",
        "<font> You should now know enough to write your own very simple scraper. Treat the boxes below as if you are writing in a completely blank colab notebook. What do you need to do to write and run your own scraper? We will be focusing on `quotes.toscrape.com`. As you can see from browsing the site, each quote has content, the author, and tags; each author also has an about page. For now, your job is write a scraper with a spider that just scrapes the content (the body of the quote itself) and the author. Make sure your spider finds the next page link and follows it.  \n",
        "</font>    "
      ],
      "metadata": {
        "id": "1EgM8P8gIimn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Make sure scrapy is installed in colab:"
      ],
      "metadata": {
        "id": "y7wFW907QVyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER CODE HERE"
      ],
      "metadata": {
        "id": "Ysvl3IobIve5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create a new scrapy project:"
      ],
      "metadata": {
        "id": "IOwIXg9pQbQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER CODE HERE"
      ],
      "metadata": {
        "id": "vS_q0-rjQOIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write your spider code to a file in the appropriate directory:"
      ],
      "metadata": {
        "id": "nDvpSRLvQikB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER CODE HERE"
      ],
      "metadata": {
        "id": "HS_gjmZEQRW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Tell scrapy to start crawling:"
      ],
      "metadata": {
        "id": "NOlAioh-Wo-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER CODE HERE"
      ],
      "metadata": {
        "id": "LACRnM_GWxa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can test out your scraper by making a new colab notebook, copy/pasting your code in the cells above and running it."
      ],
      "metadata": {
        "id": "MS51_-ibWR1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "BXqd2NtCIimp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedback\n",
        "What did you think about this notebook? What questions do you have? Were any parts confusing? Write your thoughts in the text box below.\n",
        "\n",
        "<font size =2> note: You can double click this text box in colab to edit it.</font>\n",
        "\n",
        "PUT YOUR THOUGHTS HERE"
      ],
      "metadata": {
        "id": "JOPdib-mkdbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit\n",
        "Don't forget to submit your notebook before class! Make sure you have saved your work (**Colab Menu: File-> Save**) and then download a pure python copy (**Colab Menu: File-> Download -> Download .py**)\n",
        "\n"
      ],
      "metadata": {
        "id": "-Od5aIDFnSYP"
      }
    }
  ]
}