{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xcu6MKaK0ynM",
        "rAOxOnKB3Kfz",
        "f4gBSHJR0C6E",
        "cjtuftmA6cJL",
        "3PjBVnDL6u72"
      ],
      "authorship_tag": "ABX9TyPHDN41pqI+wxz6olWMQfRA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanwalker/MGSC496/blob/main/MGSC496_L05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the in-class notebook for MGSC496 Lecture 5."
      ],
      "metadata": {
        "id": "RT9odU7BzKCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's go around the room and look at our solutions to the last exercise of R03 and the two scraping exercises of L04."
      ],
      "metadata": {
        "id": "C9ulawOlzQFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Potential Topic: Faking User Agents in Scrapy\n"
      ],
      "metadata": {
        "id": "_9FyM2Or7qk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, scrapy tells every website that you scrape that you are using scrapy. It does this for each request that is sends, through the `User-Agent` string of the request header.\n",
        "\n",
        "For example, here is the header of a request that I sent from a Scrapy shell session:\n",
        "```python\n",
        "{b'Accept': b'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        " b'Accept-Language': b'en',\n",
        " b'User-Agent': b'Scrapy/2.8.0 (+https://scrapy.org)',\n",
        " b'Accept-Encoding': b'gzip, deflate'}\n",
        "``` \n"
      ],
      "metadata": {
        "id": "q8jxwpKca2Y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The last line, where `USER_AGENT` is defined, is the fallback option if none of the over fake user agent services work. So you might want to plug in the user agent that is given for your favorite browser. For example, if you [go to this link](https://www.whatismybrowser.com/detect/what-is-my-user-agent/) you can see the user agent your browser has defined. Mine is:\n",
        ">Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\n",
        "\n",
        "You should note that, even though the websites you scrape will see these fake user agents, they still know what IP address you are accessing them from. Many sites have automated monitoring tools that look for scraper-like patterns in requests from the same IP address and may block your IP from accessing their site. To get around this, you can use proxies, which route your requests through many other proxy machines. There are [middleware libraries](https://github.com/rejoiceinhope/scrapy-proxy-pool) that you can install (with pip) and configure (in scrapy's `settings.py`) and there are both [free and paid proxy services](https://youtu.be/qHahcxoGfpc) that you can use with this middleware.\n"
      ],
      "metadata": {
        "id": "jX0uExiBfVAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Potential Topic: Items in Scrapy"
      ],
      "metadata": {
        "id": "yX_QGLolQMrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the reading, we exported data from our scraper by `yield`ing structured data items as a dictionary:\n",
        "\n",
        "```python\n",
        "class ToySpider(scrapy.Spider):\n",
        "    name = 'toyspider'\n",
        "    allowed_domains = ['cool.toys.com']\n",
        "    start_urls = ['http://cool.toys.com/new']\n",
        "    \n",
        "    def parse(self,response):\n",
        "      sel_list = response.xpath('/xpath/to/toyitem')\n",
        "      for toy_sel in sel_list:\n",
        "        toyname = toy_sel.xpath('continuedpath/to/toyname/text()').get()\n",
        "        toyprice = toy_sel.xpath('continuedpath/to/toyprice/text()').get()\n",
        "        yield {'toyname': toyname, 'toyprice': toyprice}\n",
        "```\n",
        "\n",
        "When we yield the dictionary, we are telling scrapy we want that data to go out through the data pipeline. In addition to yielding data as a dictionary, we can also [formally define our own data item objects](https://docs.scrapy.org/en/latest/topics/items.html) and yield them when we scrape. We can define one or more custom item classes in `items.py`. For example:\n",
        "\n",
        "```python\n",
        "# Inside of the file \"/content/toyscraper/toyscraper/items.py\"\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class ToyscraperItem(scrapy.Item):\n",
        "    toyname = scrapy.Field()\n",
        "    toyprice = scrapy.Field()\n",
        "```\n",
        "\n",
        "If we do this, instead of yielding a dictionary, we could instead yield an item of this type:\n",
        "\n",
        "```python\n",
        "from toyscraper.items import ToyscraperItem\n",
        "\n",
        "class ToySpider(scrapy.Spider):\n",
        "    name = 'toyspider'\n",
        "    allowed_domains = ['cool.toys.com']\n",
        "    start_urls = ['http://cool.toys.com/new']\n",
        "    \n",
        "    def parse(self,response):\n",
        "      sel_list = response.xpath('/xpath/to/toyitem')\n",
        "      for toy_sel in sel_list:\n",
        "        toyname = toy_sel.xpath('continuedpath/to/toyname/text()').get()\n",
        "        toyprice = toy_sel.xpath('continuedpath/to/toyprice/text()').get()\n",
        "        yield ToyscraperItem(toyname=toyname,toyprice=toyprice)\n",
        "```\n",
        "\n",
        "This approach allows us to:\n",
        "* specify the structure of the data we are going to extract (if we yield a dictionary, we can put any keys that we want)\n",
        "* do more advanced things with scrapy item feeds, such as:\n",
        " * validate our data (ensure that we are extracting what we think we are extracting)\n",
        " * ensure that we are not duplicating data\n",
        " * store our data\n",
        " * export our data\n",
        "and also allows more flexibility in how we extract and preprocess data  (because, for example, we can write methods of our item class to clean and sanitize the data).\n"
      ],
      "metadata": {
        "id": "w_90ImgUQQiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Scrape country info"
      ],
      "metadata": {
        "id": "xcu6MKaK0ynM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's scrape Country Data from [this site](https://www.scrapethissite.com/pages/simple/). You will notice that all of the data is on a single page, so our webscraper will not need to follow any links.\n",
        "\n",
        "First run the two code cells below:\n"
      ],
      "metadata": {
        "id": "d4rYNeJF0203"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install parsel"
      ],
      "metadata": {
        "id": "G-_gpUzvPY5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from parsel import Selector\n",
        "import requests\n",
        "res = requests.get('https://www.scrapethissite.com/pages/simple/') # grab the page using requests lib\n",
        "doc = res.text # store the html of the page in the variable doc\n",
        "selector = Selector(doc) # make a selector from doc"
      ],
      "metadata": {
        "id": "OGn2UcvTcOPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is data from 250 different countries on the page about each country's:\n",
        "* name\n",
        "* capital\n",
        "* population\n",
        "* area\n",
        "\n",
        "Use your browser inspector to inspect the html of the page and play with xpaths using `selector.xpath(...)` in the code area below to find xpaths that extract all the data described above:"
      ],
      "metadata": {
        "id": "sdvnGjn816BQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WRITE YOUR CODE TO PLAY WITH XPATHS HERE"
      ],
      "metadata": {
        "id": "esdtHBhe10vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have figured this out, make a new colab notebook called `CountryScraper.ipynb` and write all of the necessary code to:\n",
        "* install scrapy on colab\n",
        "* create a Scrapy project\n",
        "* write spider code to a file (using `%%writefile`) [ this will make use of xpaths you discovered above]\n",
        "* run the scrapy project"
      ],
      "metadata": {
        "id": "HxXG8TOQ2aiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Scrape NHL data "
      ],
      "metadata": {
        "id": "rAOxOnKB3Kfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's scape NHL data from [this site](https://www.scrapethissite.com/pages/forms/). The data is spread out across multiple pages, so your scraper will have to: 1) get all of the data from the current page; and 2) follow a link to the next page. You will not need more than one `parse` function to do this.\n",
        "\n",
        "First run the two code cells below:"
      ],
      "metadata": {
        "id": "pY7jZhRh3YPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install parsel"
      ],
      "metadata": {
        "id": "vm28RyQw3tkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from parsel import Selector\n",
        "import requests\n",
        "res = requests.get('https://www.scrapethissite.com/pages/forms/') # grab the page using requests lib\n",
        "doc = res.text # store the html of the page in the variable doc\n",
        "selector = Selector(doc) # make a selector from doc"
      ],
      "metadata": {
        "id": "8m8wOZr53tkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is data on NHL teams performance for a given year, including:\n",
        "* team name\n",
        "* year\n",
        "* wins \n",
        "* losses\n",
        "* overtimes losses\n",
        "* win percentage\n",
        "* goals for\n",
        "* goals against\n",
        "* difference (goals for - goals against)\n",
        "\n",
        "Use your browser inspector to inspect the html of the page and play with xpaths using `selector.xpath(...)` in the code area below to find xpaths that extract all the data described above:"
      ],
      "metadata": {
        "id": "CipGld-h4oNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WRITE YOUR CODE TO PLAY WITH XPATHS HERE"
      ],
      "metadata": {
        "id": "5r9tDdPF5VHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have figured this out, make a new colab notebook called `NHLScraper.ipynb` and write all of the necessary code to:\n",
        "* install scrapy on colab\n",
        "* create a Scrapy project\n",
        "* write spider code to a file (using `%%writefile`) [ this will make use of xpaths you discovered above]\n",
        "* run the scrapy project"
      ],
      "metadata": {
        "id": "KTdpgHOR52wF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# R03 Exercise: Scrape quote and author from a single page of `quotes.toscrape.com`"
      ],
      "metadata": {
        "id": "f4gBSHJR0C6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should now know enough to write your own very simple scraper. Treat the boxes below as if you are writing in a completely blank colab notebook. What do you need to do to write and run your own scraper? We will be focusing on `quotes.toscrape.com`. As you can see from browsing the site, each quote has content, the author, and tags; each author also has an about page. For now, your job is write a scraper with a spider that just scrapes the content (the body of the quote itself) and the author. "
      ],
      "metadata": {
        "id": "1EgM8P8gIimn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Make sure scrapy is installed in colab:"
      ],
      "metadata": {
        "id": "y7wFW907QVyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER CODE HERE"
      ],
      "metadata": {
        "id": "Ysvl3IobIve5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create a new scrapy project:"
      ],
      "metadata": {
        "id": "IOwIXg9pQbQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER CODE HERE"
      ],
      "metadata": {
        "id": "vS_q0-rjQOIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write your spider code to a file in the appropriate directory:"
      ],
      "metadata": {
        "id": "nDvpSRLvQikB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER CODE HERE"
      ],
      "metadata": {
        "id": "HS_gjmZEQRW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Tell scrapy to start crawling:"
      ],
      "metadata": {
        "id": "NOlAioh-Wo-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER CODE HERE"
      ],
      "metadata": {
        "id": "LACRnM_GWxa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can test out your scraper by making a new colab notebook, copy/pasting your code in the cells above and running it."
      ],
      "metadata": {
        "id": "MS51_-ibWR1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "BXqd2NtCIimp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Extend your Quote Scraper to scrape multiple pages\n"
      ],
      "metadata": {
        "id": "cjtuftmA6cJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Starting with the Quote Scraper that you wrote in the last exercise of the reading R03:\n",
        "* extend it to follow multiple pages."
      ],
      "metadata": {
        "id": "YGGt7XgRb_m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You will probably want to write this in a separate notebook"
      ],
      "metadata": {
        "id": "pkvUGFT16xGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Extend your Quote Scraper to scrape Author about pages\n"
      ],
      "metadata": {
        "id": "3PjBVnDL6u72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* now add in the ability for your scrape to gather data from each author's about page. There are many fields in the about page. Use your browser inspector and play in a scrapy shell in xterm to get the right xpaths for each field\n",
        "* Try incorporating the fake-user-agent\n",
        "* Use logging to write the user-agent, which can be found in `response.request.headers`to the log. \n",
        "\n"
      ],
      "metadata": {
        "id": "QcJi7pAVb3gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You will probably want to write this in a separate notebook"
      ],
      "metadata": {
        "id": "1sYJFBbe6_ul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}