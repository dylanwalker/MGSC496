{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "atSB9TXzYakl",
        "q3ug_VFBXbqu"
      ],
      "authorship_tag": "ABX9TyMBM9AIEnUKobuQFsRL1oFL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanwalker/MGSC496/blob/main/MGSC496_L04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the in-class notebook for MGSC496 Lecture 4."
      ],
      "metadata": {
        "id": "dA_4fackixhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use this space to go over examples, try some exercises, do live coding, and work through any of questions the class might have."
      ],
      "metadata": {
        "id": "clxCFGyVi6vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install parsel"
      ],
      "metadata": {
        "id": "4RT2D-6nn4_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from parsel import Selector\n",
        "import requests\n",
        "page = requests.get('http://quotes.toscrape.com') # fetch the webpage \"http://quotes.toscrape.com\"\n",
        "doc = page.text # store the raw html of the page\n",
        "selector = Selector(doc) # make an xpath selector object from the raw html"
      ],
      "metadata": {
        "id": "e6AO21oqnygG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\">Use xpath to get a list of the authors of all the quotes</font>\n",
        "\n",
        "<font> Using your browser inspector and python code find a way to:\n",
        "\n",
        "    * create a list of all of the authors of the quotes.\n",
        "    * create a list of all of the links to about for each author\n",
        "</font>    "
      ],
      "metadata": {
        "id": "ZDV8c-xtmKOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the xpath that we get using copy->xpath:\n",
        "# # /html/body/div/div[2]/div[1]/div[1]/span[2]/small\n",
        "selector.xpath('//div[@class=\"quote\"]/span[@class=\"text\"]/text()').get()\n",
        "selector.xpath('//span[@class=\"text\"]/text()').get()\n",
        "#selector.xpath('//div[@class=\"quote\"]/span/small/text()').getall()\n",
        "#selector.xpath('//div[@class=\"quote\"]//a')\n",
        "selector.xpath('//a[text()=\"(about)\"]')\n",
        "selector.xpath('//a[contains(text(),\"about\")]')"
      ],
      "metadata": {
        "id": "Mpb92hkrmKOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "25DcACrFmKOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"\"\"\n",
        "<html><body>\n",
        "<div class=\"toyblock\">\n",
        "  <span class=\"imgwrapper\"><img src=\"voltron.png\" /></span>\n",
        "  <span class=\"toyname\">Voltron</span>\n",
        "  <span class=\"price\">$14.99</span>\n",
        "</div>\n",
        "<div class=\"toyblock\">\n",
        "  <span class=\"imgwrapper\"><img src=\"he-man.png\" /></span>\n",
        "  <span class=\"toyname\">He-Man</span>\n",
        "  <span class=\"price\">$5.99</span>\n",
        "</div>\n",
        "</body></html>\n",
        "\"\"\"\n",
        "selector = Selector(doc)\n",
        "selector.xpath('//span[@class=\"toyname\"]/text()').getall() # get a list of the inner text of span tags with class=\"toyname\""
      ],
      "metadata": {
        "id": "XQHSXU_jtZdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\"> Test Your Xpath</font>\n",
        "\n",
        "<font> Using the same document as in the above code, write a single line of code that generates a list of the image .png filenames for the toys on the page.   \n",
        "</font>    "
      ],
      "metadata": {
        "id": "EJxE9h44Ezxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here\n",
        "selector.xpath('//span[@class=\"imgwrapper\"]/img/@src').getall()\n",
        "#selector.xpath('//img/@src').getall()"
      ],
      "metadata": {
        "id": "2BPFGABIFAWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "8Nv7bk_IEzxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recap: L03 Exercises -- Questions?\n",
        "\n",
        "Let's see YOUR solutions to the exercises in L03"
      ],
      "metadata": {
        "id": "tW6OsHxhZi9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Can someone show how they implemented passing `timeToStarve` and `huntingProwess`when creating an object of the `Spider` class?\n",
        "\n",
        "* Can someone show an implementation for a `Bee` class?\n",
        "\n",
        "* Can someone show their approach to the RPG Character Class? "
      ],
      "metadata": {
        "id": "sraXpU-5veuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recap: Let's go over the reading R03. Questions?"
      ],
      "metadata": {
        "id": "Zh_h-qt5bneH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Do we feel comfortable with using xpath selectors?\n",
        "\n",
        "* Do we understand the architecture of Scrapy?\n",
        "\n",
        "* Do we understand the components of a Scrapy scraper project?\n",
        "\n",
        "* Do we understand how to make our own Scrapy project in a blank notebook?"
      ],
      "metadata": {
        "id": "5xOTsvV8wDay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Legality and Ethics of Scraping\n",
        "\n",
        "Is webscraping legal? Who the heck knows? This isn't entirely settled law. However, currently [the recent court case of HiQ v LinkedIn](https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn) from the 9th district court point to yes. Yet, another case [(Meta v. Social Data Trading Ltd.)](https://www.techdirt.com/2022/01/11/meta-sues-firm-data-scraping-claims-that-signing-up-new-accounts-after-being-banned-is-equivalent-hacking/) reveals that the answer is much more subtle and depends on the methods of scraping, what data was accessed, and *how* it was accessed, as well as regional laws. Interestingly, many of the tech platforms prosecuting these cases have themselves used and continue to use scraping, in addition to selling data to third parties.  \n",
        "\n",
        " Most legal scholars agree (untested regional laws aside) that the federal legality of datascraping requires:\n",
        "* the data we are scraping is not copyrighted content\n",
        "* the data is publicly available\n",
        "* the data that is not otherwise specially protected by law (see the [California Rights Privacy Act](https://en.wikipedia.org/wiki/California_Privacy_Rights_Act), [EU General Data Protection Regulation](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation)). \n",
        "\n",
        "Use of webscraping by/for a business should be preceeded by careful review of the laws, potential risks and liabilities. Webscraping for education and personal learning projects likely does not require such extensive scrutiny.\n",
        "\n",
        "\n",
        "There are also many ethical considerations, such as:\n",
        "* Is the data we are scraping data about people? \n",
        "* If so, how might they feel about us scraping the data?\n",
        "* Could our data collection be subjecting people to violations of privacy or creating the potential for harm?\n",
        "* Would our data scraping benefit society in any way?\n",
        "* Would our scraping inhibit, unduly harm, incur costs on, or affect the normal operations of the sites that we are scraping?\n",
        "* How can we be \"good net citizens\" when we scrape?\n",
        "\n"
      ],
      "metadata": {
        "id": "e9NdPJXnAWa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Potential Topic: Your Spider has access to the logger\n",
        "\n"
      ],
      "metadata": {
        "id": "wXozXqHLSA8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This is a really quick topic, just to let you know that your spider has access to the scrapy logger (because your spider inherits from `scrapy.Spider`). You can use it to write things out to the log. For example, in a parse function within your spider, you can call:\n",
        "\n",
        "```python\n",
        "class SomeSpider(scrapy.Spider):\n",
        "    name = 'somename'\n",
        "    allowed_domains = ['somedomain']\n",
        "    start_urls = ['http://somedomain.com/start']\n",
        "\n",
        "    def parse(self, response):\n",
        "      #do something here\n",
        "      self.logger.info(\"This message will end up in the log.\")\n",
        "```\n",
        "\n",
        "Scrapy uses the standard logging library that comes with python. Using this logging library is a mini-topic unto itself, but for now, know that you can use it to write messages to the log, which might go to the console (default) or to a log file, if you add the option `--logfile somefilename.log` to your `scrapy crawl` [shell command](https://docs.scrapy.org/en/latest/topics/logging.html#command-line-options). \n",
        "\n",
        " There are [different levels of logger messages](https://docs.scrapy.org/en/latest/topics/logging.html#log-levels), and we can [use these](https://docs.scrapy.org/en/latest/topics/logging.html#how-to-log-messages) for individuals messages and in conjunction with [setting the log level](https://docs.scrapy.org/en/latest/topics/logging.html#command-line-options) (with `--loglevel LEVEL` in our `scrapy crawl` command). For example, we might want to log some messages when we are debugging our program that we would not want to log when we are actually using our program. If we set the logging level higher, than lower messages will not appear in the log.  "
      ],
      "metadata": {
        "id": "VA-UxhG3a7bX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "liAwWW9ESApJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Potential Topic: Faking User Agents in Scrapy\n"
      ],
      "metadata": {
        "id": "_9FyM2Or7qk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "By default, scrapy tells every website that you scrape that you are using scrapy. It does this for each request that is sends, through the `User-Agent` string of the request header.\n",
        "\n",
        "For example, here is the header of a request that I sent from a Scrapy shell session:\n",
        ">```python\n",
        "{b'Accept': b'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        " b'Accept-Language': b'en',\n",
        " b'User-Agent': b'Scrapy/2.8.0 (+https://scrapy.org)',\n",
        " b'Accept-Encoding': b'gzip, deflate'}\n",
        "``` \n",
        "\n",
        "Notice that the `User-Agent` string is set to `'Scrapy/2.8.0 (+https://scrapy.org)'`.\n",
        "\n",
        "This can cause some webistes to block our attempts to scrape them.  **Does this mean we don't have a right to scrape them?**\n",
        "\n",
        "Many use fake user agents to avoid having their scraping attmepts shut down. One way to do this is by using the [scrapy-fake-useragent](https://github.com/alecxe/scrapy-fake-useragent) library.\n",
        "\n",
        "We can install this by running the shell command:\n",
        "```\n",
        "!pip install scrapy-fake-useragent\n",
        "```\n",
        "\n",
        "To use this library, we will have to add these lines to our scraper `settings.py` file:\n",
        "```python\n",
        "DOWNLOADER_MIDDLEWARES = {\n",
        "    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
        "    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n",
        "    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n",
        "    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n",
        "}\n",
        "FAKEUSERAGENT_PROVIDERS = [\n",
        "    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # this is the first provider we'll try\n",
        "    'scrapy_fake_useragent.providers.FakerProvider',  # if FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us\n",
        "    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # fall back to USER_AGENT value\n",
        "]\n",
        "USER_AGENT = '<your user agent string which you will fall back to if all other providers fail>'\n",
        "```\n",
        "\n",
        "The last line, where `USER_AGENT` is defined, is the fallback option if none of the over fake user agent services work. So you might want to plug in the user agent that is given for your favorite browser. For example, if you [go to this link](https://www.whatismybrowser.com/detect/what-is-my-user-agent/) you can see the user agent your browser has defined. Mine is:\n",
        ">Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\n",
        "\n",
        "You should note that, even though the websites you scrape will see these fake user agents, they still know what IP address you are accessing them from. Many sites have automated monitoring tools that look for scraper-like patterns in requests from the same IP address and may block your IP from accessing their site. To get around this, you can use proxies, which route your requests through many other proxy machines. There are [middleware libraries] (https://github.com/rejoiceinhope/scrapy-proxy-pool) that you can install (with pip) and configure (in scrapy's `settings.py`) and there are both [free and paid proxy services](https://youtu.be/qHahcxoGfpc) that you can use with this middleware.\n"
      ],
      "metadata": {
        "id": "q8jxwpKca2Y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Potential Topic: Items in Scrapy"
      ],
      "metadata": {
        "id": "yX_QGLolQMrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the reading, we exported data from our scraper by `yield`ing structured data items as a dictionary:\n",
        "\n",
        "```python\n",
        "class ToySpider(scrapy.Spider):\n",
        "    name = 'toyspider'\n",
        "    allowed_domains = ['cool.toys.com']\n",
        "    start_urls = ['http://cool.toys.com/new']\n",
        "    \n",
        "    def parse(self,response):\n",
        "      sel_list = response.xpath('/xpath/to/toyitem')\n",
        "      for toy_sel in sel_list:\n",
        "        toyname = toy_sel.xpath('continuedpath/to/toyname/text()').get()\n",
        "        toyprice = toy_sel.xpath('continuedpath/to/toyprice/text()').get()\n",
        "        yield {'toyname': toyname, 'toyprice': toyprice}\n",
        "```\n",
        "\n",
        "When we yield the dictionary, we are telling scrapy we want that data to go out through the data pipeline. In addition to yielding data as a dictionary, we can also [formally define our own data item objects](https://docs.scrapy.org/en/latest/topics/items.html) and yield them when we scrape. We can define one or more custom item classes in `items.py`. For example:\n",
        "\n",
        "```python\n",
        "# Inside of the file \"/content/toyscraper/toyscraper/items.py\"\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class ToyscraperItem(scrapy.Item):\n",
        "    toyname = scrapy.Field()\n",
        "    toyprice = scrapy.Field()\n",
        "```\n",
        "\n",
        "If we do this, instead of yielding a dictionary, we could instead yield an item of this type:\n",
        "\n",
        "```python\n",
        "from toyscraper.items import ToyscraperItem\n",
        "\n",
        "class ToySpider(scrapy.Spider):\n",
        "    name = 'toyspider'\n",
        "    allowed_domains = ['cool.toys.com']\n",
        "    start_urls = ['http://cool.toys.com/new']\n",
        "    \n",
        "    def parse(self,response):\n",
        "      sel_list = response.xpath('/xpath/to/toyitem')\n",
        "      for toy_sel in sel_list:\n",
        "        toyname = toy_sel.xpath('continuedpath/to/toyname/text()').get()\n",
        "        toyprice = toy_sel.xpath('continuedpath/to/toyprice/text()').get()\n",
        "        yield ToyscraperItem(toyname=toyname,toyprice=toyprice)\n",
        "```\n",
        "\n",
        "This approach allows us to:\n",
        "* specify the structure of the data we are going to extract (if we yield a dictionary, we can put any keys that we want)\n",
        "* do more advanced things with scrapy item feeds, such as:\n",
        " * validate our data (ensure that we are extracting what we think we are extracting)\n",
        " * ensure that we are not duplicating data\n",
        " * store our data\n",
        " * export our data\n",
        "and also allows more flexibility in how we extract and preprocess data  (because, for example, we can write methods of our item class to clean and sanitize the data).\n"
      ],
      "metadata": {
        "id": "w_90ImgUQQiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Potential Topic: Data Classes"
      ],
      "metadata": {
        "id": "atSB9TXzYakl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember in the reading, when I talked about how before classes there were structs? Those were objects designed to group \"like variables\" together in a common structure. Well in Python, we can do that with data classes. We can make a dataclass and get a \"whole bunch of convenient, boilerplate class code for free\" by unsing the @dataclass decorator:"
      ],
      "metadata": {
        "id": "ZP87nRuG-O96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Point:\n",
        "  x: float # This is an example of \"type hinting\", where we hint at what the type of a property might be. (remember, python is loosely typed)\n",
        "  y: float\n",
        "  z: float\n",
        "\n",
        "p1 = Point(1.0,5.0,2.4)\n",
        "p2 = Point(2.2,1.1,8.3)\n",
        "p3 = Point(1.0,5.0,2.4)\n",
        "\n",
        "print(p1)\n",
        "print(p2)\n",
        "print(p3)\n",
        "p1 == p3"
      ],
      "metadata": {
        "id": "p_hUD_P9Ydun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data classes enable us to do lots of things. We can:\n",
        " * set default values for properties of a dataclass\n",
        " * decide whether a datclass object is mutable or not\n",
        " * decide what data properties will be considered when we test for equality of two dataclass objects\n",
        " * decided whether a proprety should be printed when we print a dataclass object\n",
        " * decide which property should be used to sort dataclass objects.\n",
        " \n",
        "They are a relatively new addition to Python. You can read more about them [here](https://www.dataquest.io/blog/how-to-use-python-data-classes/), watch a video showing why they are great [here](https://youtu.be/vBH6GRJ1REM) or check the python docs on dataclasses [here](https://docs.python.org/3/library/dataclasses.html). "
      ],
      "metadata": {
        "id": "AB2kBwQ5kwco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Potential Topic: Class Properties and Setters"
      ],
      "metadata": {
        "id": "q3ug_VFBXbqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python doesn't really have a notion of **private** properties of a class. The idea is that private properties are those that can't be accessed by other parts of the code (only the objects of a class can access their own private data members). THough, python does private a way to partially achieve this, and it does so by using a double underscore (dunder) in front of the property. For example if we had an object `person1` of class `Person` and we declared the property `__name`, then we wouldn't be able to access it (to read its value or to change its value by using `person1.__name` in our code.\n",
        "\n",
        "Look at this example code below:"
      ],
      "metadata": {
        "id": "NIhrQyQnhxmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Person:\n",
        "  def __init__(self,name):\n",
        "    self.__name = name\n",
        "\n",
        "person1 = Person(\"John\")\n",
        "#print(person1.__name) # This line would not work.\n"
      ],
      "metadata": {
        "id": "VcK--c5QYexj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# However, we can see that actually, we still do have acces to this. We can look at `__dict__` which stores all of the things that below to an object\n",
        "print(person1.__dict__) # So we can actually see that __name is still accessible to us\n"
      ],
      "metadata": {
        "id": "KboKNw6Dnoif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the property seems to be stored as `_Person__name`. This technique partially discourages a user of our code from trying to access and change a property that we want to be private. But it can be circumvented:"
      ],
      "metadata": {
        "id": "TyJUQGzBir7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "person1._Person__name = \"Joe\"\n",
        "print(person1.__dict__)"
      ],
      "metadata": {
        "id": "zLzK2K2Tio9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why would we want to do this? Well sometimes we want to control what happens when a property is changed. Suppose we were a businesss that was tracking our employees and we would not want code to be able to change an employee name without also doing a bunch of other stuff in our records system. \n",
        "\n",
        "We can control what happens when a \"private\" property of a class is read or altered by using the `@property` decorator and a `someProperty.setter` decorator, like this: "
      ],
      "metadata": {
        "id": "lCBtaT3ii5HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Person:\n",
        "  def __init__(self,name):\n",
        "    self.__name = name\n",
        "\n",
        "  @property\n",
        "  def name(self):\n",
        "    # Some code here that does soemthing whenever the name is read\n",
        "    print(\"Executing code when name is accessed.\")\n",
        "    return self.__name\n",
        "  \n",
        "  @name.setter\n",
        "  def name(self, new_name): #note that normally we can never define two methods that have the same name, but with the @property.setter decorator, we can.\n",
        "    # Some code here that does soemthing whenever the name is changed\n",
        "    print(\"Executing code when name is changed.\")\n",
        "    self.__name = new_name\n",
        "\n",
        "p1 = Person(\"John\")\n",
        "print(p1.name)\n",
        "p1.name = \"Joe\"\n",
        "  "
      ],
      "metadata": {
        "id": "qwu6tUIpjUxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Extend your Quote Scraper to scrape multiple pages\n"
      ],
      "metadata": {
        "id": "cjtuftmA6cJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Starting with the Quote Scraper that you wrote in the last exercise of the reading R03:\n",
        "* extend it to follow multiple pages."
      ],
      "metadata": {
        "id": "YGGt7XgRb_m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You will probably want to write this in a separate notebook"
      ],
      "metadata": {
        "id": "pkvUGFT16xGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Extend your Quote Scraper to scrape Author about pages\n"
      ],
      "metadata": {
        "id": "3PjBVnDL6u72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* now add in the ability for your scrape to gather data from each author's about page. There are many fields in the about page. Use your browser inspector and play in a scrapy shell in xterm to get the right xpaths for each field\n",
        "* Try incorporating the fake-user-agent\n",
        "* Use logging to write the user-agent, which can be found in `response.request.headers`to the log. \n",
        "\n"
      ],
      "metadata": {
        "id": "QcJi7pAVb3gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You will probably want to write this in a separate notebook"
      ],
      "metadata": {
        "id": "1sYJFBbe6_ul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}