{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanwalker/MGSC496/blob/main/MGSC496_R06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Your Info\n",
        "\n",
        "your_name = '' #@param {type:\"string\"}\n",
        "your_email = '' #@param {type:\"string\"}\n",
        "today_date = '' #@param {type:\"date\"}\n"
      ],
      "metadata": {
        "id": "qCErBFYSbdOw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to \"read\" this notebook\n",
        "\n",
        "As you go through this notebook (or any notebook for this class), you will encounter new concepts and python code that implements them -- just like you would see in a textbook. Of course, in a textbook, it's easy to read code and an explanation of what it does and think that you understand it.\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "### Learn by doing\n",
        "But this notebook is different from a textbook because it allows you to not just read the code, but play with it. **You can and should try out changing the code that you see**. In fact, in many places throughout this reading notebook, you will be asked to write your own code to experiment with a concept that was just covered. This is a form of \"active reading\" and the idea behind it is that we really learn by **doing**. \n",
        "<br />\n",
        "<br />\n",
        "\n",
        "### Change everything\n",
        "But don't feel limited to only change code when I prompt you. This notebook is your learning environment and your playground. I encourage you to try changing and running all the code throughout the notebook and even to **add your own notes and new code blocks**. Adding comments to code to explain what you are testing, experimenting with or trying to do is really helpful to understand what you were thinking when you revisit it later. \n",
        "<br />\n",
        "<br />\n",
        "### Make this notebook your own\n",
        "Make this notebook your own. Write your questions and thoughts. At the end of every reading notebook, I will ask the same set of questions to try to elicit your questions, reaction and feedback. When we review the reading notebook in class, I encourage you to   \n",
        "\n"
      ],
      "metadata": {
        "id": "LHv80meh7uAs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swByiBfML8m_"
      },
      "source": [
        "# Code Preface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTfOe5Ymb_Zn"
      },
      "source": [
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvfQRYqvb_Z5"
      },
      "source": [
        "# scikit-learn: Machine Learning in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVV6S587b_Z_"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=19uME3dIsvBDOtVg5eDeIN7tmLLiRpjwx\" width=300>\n",
        "\n",
        "\n",
        "Scikit Learn (sklearn for short) is a machine learning library in Python that provides access to many common [ML algorithms](https://scikit-learn.org/stable/index.html). You've already seen supervised and unsupervised ML algorithms. So in this lecture, we'll focus on the basics of the sklearn APIs.  \"scikit\" refers to \"scipy toolkit\". \n",
        "\n",
        "The six main categories of scikit-learn algorithms are:\n",
        "* Regression\n",
        "* Classification\n",
        "* Clustering\n",
        "* Dimensionality reduction\n",
        "* Model selection\n",
        "* Preprocessing\n",
        "\n",
        "Data (features and targets) is passed into sklearn algorithms typically as Pandas dataframes or numpy arrays or even python lists. \n",
        "\n",
        "We'll first discuss how to us sklearn using artificial data, as this will simplify the basic concepts of fitting different models to data. We will follow this with a discussion of steps, tips and tricks for using sklearn with real data.\n",
        "\n",
        "We'll start by talking about the Estimator API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nJTFOYlb_aH"
      },
      "source": [
        "## Estimators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3i2SEe6b_aN"
      },
      "source": [
        "In scikit-learn, a machine learning model is called as **Estimator**.\n",
        "\n",
        "Each **Estimator** is a Python `class` and has a form like this:\n",
        "\n",
        "```python\n",
        "class estimator():\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def fit(self):\n",
        "        # do some calculations with self.data\n",
        "```\n",
        "\n",
        "Most commonly, the steps in using the Scikit-Learn Estimator API are as follows:\n",
        "\n",
        "1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn.\n",
        "2. Choose model hyperparameters by instantiating this class with desired values.\n",
        "3. Arrange data into a features matrix and target vector.\n",
        "4. Fit the model to your data by calling the ``fit()`` method of the model instance.\n",
        "5. Apply the Model to new data:\n",
        "   - For supervised learning, often we predict labels for unknown data using the ``predict()`` method.\n",
        "   - For unsupervised learning, we often transform or infer properties of the data using the ``transform()`` or ``predict()`` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyDqKd7jPam8"
      },
      "source": [
        "\n",
        "So if you want to estimate coefficients or learn patterns within data, simply do the following:\n",
        "\n",
        "1. initialize an estimator.\n",
        "2. Fit the estimator with data of your interest.\n",
        "\n",
        "We'll walk through some very common ML algorithms, starting with regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3efFIkl6b_aS"
      },
      "source": [
        "## Regression : Linear Regression - Ordinary Least Squares (OLS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2TON8pjb_aU"
      },
      "source": [
        "You should all be familiar with the concept of regression. In regression, we have a set of *independent* variables:\n",
        "\n",
        "$x_1, x_2, ..., x_p$\n",
        "\n",
        "and a *dependent* variable:\n",
        "\n",
        "$y$\n",
        "\n",
        "We might have independent and dependent variables in dataframe, where the rows are the different data points and the independent variables or **features** of each data point are the columns $x_i$ and the **outcome** (also called the **target**) $y$ is a column.\n",
        "\n",
        "Our goal is to learn a model that \"predicts\" the outcome $\\hat{y}$. Every model has parameters or weights that can be adjusted or tuned in order to best **fit** the model to the data -- we call this procedure \"training the model\" or \"fitting the model to the data\".\n",
        "\n",
        "For Ordinary Least Squares regression, our model is:\n",
        "\n",
        "$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + ... + w_p x_p$\n",
        "\n",
        "and the parameters or weights are $w_0,w_1,...,w_p$.  \n",
        "\n",
        "This is often written in a more compact way in vector/matrix notation:\n",
        "\n",
        "$\\hat{y} = Xw$\n",
        "\n",
        "$\\hat{y}$ is the predicted outcome (we'll often use `y_pred` to denote this in our code). In other words, for a given set of weights, the model will predict the outcome $\\hat{y}$ for a data point, while the actual outcome of that data point is $y$. They will not necessarily be the same, because our model will not fit the data perfectly. Each data point will have an error $\\epsilon = y-\\hat{y}$\n",
        "\n",
        "\n",
        "Learning the model is all about finding a set of weights $w_i$ that minimizes the error from all data points: \n",
        "\n",
        "$\\sum_{data\\ points}{(y-Xw)^2}$\n",
        "\n",
        "\n",
        "To accomplish this in sklearn, we'll start by importing `linear_model` and initializing it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50xyPomib_aY"
      },
      "source": [
        "# initialize a linear model estimator\n",
        "from sklearn import linear_model # import model\n",
        "lm = linear_model.LinearRegression() # instantiate model (no hyperparameters here)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FVlKQ6Yb_aj"
      },
      "source": [
        "`LinearRegression` accepts two inputs $X$ and $y$.\n",
        "\n",
        "We'll typically use dataframes or 2d numpy arrays (though the shape depends on whether each feature $x$ is a single number of a vector), so we'll try to use the convention that the first dimension is the rows(or data points) and 2nd dimension is the columns or features.\n",
        "\n",
        "For example, suppose we 5 data points, a single 1d features $x$ for each data point, and a single 1d outcome $y$ for each data point. Then:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$X = [[x_{11}], [x_{21}],[x_{31}]]$\n",
        "\n",
        "y = $[y_{1}, y_{2}, y_{3}]$\n",
        "\n",
        "More generally, the shape of $X$ is `(n_samples,n_features)` and the shape of $y$ is `(n_samples)` or, if we have multiple outcomes for each data point, `(n_samples, n_outcomes)`. (here, `n_samples` is the number of data points).\n",
        "\n",
        "Here's a simple example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pg_t03Fb_ar"
      },
      "source": [
        "# A simple examples of data features and outcomes\n",
        "X = np.array([[1], [2], [3], [4], [5]]) # five samples or data points, each has a single 1d feature\n",
        "y = np.array([0,2,4,1,4]) # five samples or data points, each has a single 1d outcome\n",
        "print(f'Input features X have shape {X.shape}')\n",
        "print(f'Outcomes y have shape {y.shape}')\n",
        "plt.scatter(X, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz8cak8zb_a0"
      },
      "source": [
        "Now, we can fit the estimator `lm` like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xihWeA-b_a4"
      },
      "source": [
        "lm.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imNo2r_eb_a-"
      },
      "source": [
        "Estimated intercept $w_0$ and coefficients $w_i$ are stored in `lm.intercept_` and `lm.coef_` respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WphayXjb_bA"
      },
      "source": [
        "print(f'Estimated intercept is {lm.intercept_} and estimated coefficient is {lm.coef_[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcba9Obzb_bI"
      },
      "source": [
        "The estimated linear model is $Y = 0.1 + 0.7X$.\n",
        "\n",
        "Once we have fitr the model we'll use the model's `predict()` method to get a prediction for new data.\n",
        "\n",
        "For example, if you want to predict a value for $x=10$? &rarr; we can use `lm.predict()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyTqGrdfb_bL"
      },
      "source": [
        "lm.predict(np.array([[10]])) # be sure that the X you use with predict is a two dimensional array (hence the double square brackets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o-k1cbub_bT"
      },
      "source": [
        "Lastly, we can use `lm.predict()`, to get the predicted outcome for the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JHMmMD4b_bV"
      },
      "source": [
        "y_pred = lm.predict(X)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, y_pred, color='gray', marker='o', linestyle='--');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5idZnEJxb_bd"
      },
      "source": [
        "Notice in the above that the gray line that our model learned is the one that minimizes the error across all data points. The gray dot is the predicted outcome $\\hat{y}$, while the blue dot is actual outcome $y$.\n",
        "\n",
        "\n",
        "Remember the rules of scikit-learn. Initialize and fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghZFtG5Tb_bf"
      },
      "source": [
        "## Classification : Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii6xOeQUb_bj"
      },
      "source": [
        "A [Support Vector Machine (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine) is a supervised learning model that classifies data points into a set of given classes or labels.\n",
        "\n",
        "SVM finds \"hyperplanes\" (think of the higher dimensional extension of a line dividing two regions of space in two dimension) that maximally divides data into the labelled groups. All data on one side of the hyperplane gets one label (or class), all data on the other side gets a different label (or class). Therefore we say the model \"classifies\" the data. \n",
        "\n",
        "* For $p$ dimensional vectors, its hyperplane of $(p-1)$ dimensions can separate the vectors into labels.\n",
        "* For example, if each observation has two values, a hyperplane that divides observations is a line (1-dim).\n",
        "* If each observation has three values, a hyperplane is a plane (2-dim).\n",
        "\n",
        "A hyperplane that divides data points can be expressed as $\\overrightarrow{w}$ that satisfies $\\overrightarrow{w}\\overrightarrow{x}-b = c$ where $c$ is a value between two labels. But don't worry about this mathematical description, let's see how it works:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoAJhuUDb_bl"
      },
      "source": [
        "### SVM Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RmvBbM3b_bn"
      },
      "source": [
        "To illustrate this, we'll create an artificial dataset by randomly generating samples from two multivariate normal distributions. The \"class\" or \"label\" in this case is the distribution from which it is drawn (in this example, there are two different distributions that we draw points from, so there are two classes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TYXxWxJb_bq"
      },
      "source": [
        "np.random.seed(1) # set the seed, so we all get the same random draws and therefore have the same data.\n",
        "dat1 = np.random.multivariate_normal(mean=[1,1], cov=[[0.3, 0], [0, 0.3]], size=50)\n",
        "dat2 = np.random.multivariate_normal(mean=[2,1.5], cov=[[0.3, 0], [0, 0.3]], size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blw0UIeab_bv"
      },
      "source": [
        "# Visualize the datapoints drawn from these two distributions\n",
        "plt.scatter(dat1[:,0], dat1[:,1]);\n",
        "plt.scatter(dat2[:,0], dat2[:,1]);\n",
        "plt.xlabel('$x_1$');\n",
        "plt.ylabel('$x_2$');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice each data point has a 2d feature $(x_1,x_2)$ that is plotted. The color of the datapoint refers to which class it belongs to (i.e., which distribution it was drawn from).\n",
        "\n",
        "We'll combine these to make a single dataset:"
      ],
      "metadata": {
        "id": "cFBH1JeZv-B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.concatenate((dat1, dat2)) # The first 50 rows are from the first 2d normal distribution, the next 50 rows are from the second 2d normal distribution\n",
        "y = [0]*50 + [1]*50 # This is just a quick way to make the labels by repeating the elements of two lists and concatenating them together. \n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "UwKaN4q-v7-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Input features X have shape {X.shape}')\n",
        "print(f'Outcomes y have shape {y.shape}')"
      ],
      "metadata": {
        "id": "BHDtsvbtwFN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp2xLy2Nb_b0"
      },
      "source": [
        "To proceed, we simply follow the steps, starting with:\n",
        "\n",
        "1. Initialize an estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKA4iFLnb_b3"
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.LinearSVC() # There are different types of SVMs, here we'll use a Linear support vector machine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRSb9qudb_b_"
      },
      "source": [
        "2. Fit the estimator to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8PrqkZyb_cA"
      },
      "source": [
        "clf.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfEdcQ-4SuuT"
      },
      "source": [
        "To see what the model arrived at for the fit, we can print the coefficients:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoDf07-eb_cJ"
      },
      "source": [
        "print(clf.intercept_)\n",
        "print(clf.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P78uDG0pb_cS"
      },
      "source": [
        "Thus, the learned classifier is $-2.87 + 1.30x_{1}+0.67x_{2}$.\n",
        "\n",
        "And we can make predictions using this model for new data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv2mxrlbb_cU"
      },
      "source": [
        "Xnew = np.array([[0.5, 0], [1.5, 3], [3, 2]])\n",
        "ynew_pred = clf.predict(Xnew)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmNBNVDeb_ca"
      },
      "source": [
        "#### Check the results on a plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqgOJV4_TBTP"
      },
      "source": [
        "Let's visualise the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdrs_uMzb_ch"
      },
      "source": [
        "plt.scatter(dat1[:,0], dat1[:,1], alpha=0.2);\n",
        "plt.scatter(dat2[:,0], dat2[:,1], alpha=0.2);\n",
        "\n",
        "X_tmp = np.arange(0.5, 2.5, 0.1) # generate tickpoints along the x axis, so we can use them to draw the estimated line\n",
        "SVM_line = 1/clf.coef_[0][1]*(-clf.intercept_[0] - clf.coef_[0][0]*X_tmp) # get the dividing line\n",
        "plt.plot(X_tmp, SVM_line, color='gray', linestyle='--', label='SVM');\n",
        "\n",
        "\n",
        "plt.scatter(Xnew[:,0], Xnew[:,1], marker='s', s=100, \n",
        "            color = ['tab:blue' if pred==0 else 'tab:orange' for pred in ynew_pred], label='Predicted'); # plot the predicted data for our three new datapoints\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above plot the faded blue and orange dots are the data that we used to train the SVM classifier. The dashed line is the line that the model learned that best divides the data into the two classes. The blue and orange boxes are new data (data the model never saw during training) that the model classifies. "
      ],
      "metadata": {
        "id": "3TYb7SovynUh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRExyPGEb_co"
      },
      "source": [
        "### Example: Let's build a SVM classifier that predicts who survived the Titanic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWChR1-Db_cq"
      },
      "source": [
        "We'll start by looking at some data on who survived the titanic. The full dataset is included as part of the `seaborn` package, but we'll use a version that I've reduced and cleaned up a bit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK8q7MAwb_ct"
      },
      "source": [
        "titanicFile='https://raw.githubusercontent.com/dylanwalker/MGSC496/main/datasets/titanic_cleaned.csv'\n",
        "titanic = pd.read_csv(titanicFile)\n",
        "titanic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emsQBWJRBnbQ"
      },
      "source": [
        "X = titanic.loc[:,'pclass':'fare'] # select all rows, and all columns starting with 'pclass' and ending with 'fare'\n",
        "y = titanic.loc[:,'survived'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "jRdhdQWPzl6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "81gtUz9Gzn-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfAaLpMbb_c0"
      },
      "source": [
        "We'll build a model using the pclass (passenger class), sex, age, and fare (the fair they paid) to try to predict survived (whether the passenger survived)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6h03xHNb_dM"
      },
      "source": [
        "First, we'll initiate an SVM model. Unlike the previous example, we'll use the SVC type that provides some advanced kernels. I won't talk about these in detail, though you can read about SVM kernels [here](https://data-flair.training/blogs/svm-kernel-functions/), except to say that nonlinear kernels permit finding divisions in the data beyond a simple line dividing the space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsEkAsNsb_dO"
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(gamma='auto', random_state=0) # SVC covers not only linear kernel as LinearSVC but also nonlinear kernels\n",
        "clf.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd2qqqk1YzQR"
      },
      "source": [
        "You might have noticed in the above that we set `random_state=0` when we instantiated the `SVC` model. You might be wondering what role random numbers play in the `SVC` model. SVC uses a method called cross-fold validation internally when training, which involves some shuffling of the data. We'll talk more about this in just a minute. We set the `random_state` to a specified seed to ensure that anyone running this code would get the same results. If we were doing this in a real-world application, we would not do this.\n",
        "\n",
        "\n",
        "In order to evaluate an ML model, there are a variety of metrics that you should already be aware of (e.g., accuracy, precision, recall, F-score, etc.). We'll use accuracy, which we can import from `sklearn.metrics`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QB4tdR6b_dT"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = clf.predict(X)\n",
        "print('{:.2%}\\n'.format(accuracy_score(y, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS0eATPsZE7S"
      },
      "source": [
        "So you might think this model performs pretty well. However, this is not a good assessment of the model, because we assessed performance on the training data. This is a meaningless measure of performance. As in all machine learning, we need to split data into training and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2mxsUwZb_dZ"
      },
      "source": [
        "## Model selection : How good is our model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkw89ZJbb_df"
      },
      "source": [
        "Up until this point, we used the entire data to learn the model. However, you know that this is not the correct approach. We need to holdout some data in order to evaluate the model that we learned on \"unseen data\".\n",
        "\n",
        "A typical step after loading data is to split it into train, validation, and test sets:\n",
        "\n",
        "* **Training data**: a subset of data to train a model\n",
        "* **Validation data**: a subset of data to pick the best hyperparameters of a model or compare across models\n",
        "* **Test data**: a subset of data to evaluate the chosen learned model\n",
        "\n",
        "We won't be tuning model hyperparameters or choosing between models right now, so we'll just split the data into a 70\\% training set and 30\\% as test set.  \n",
        "\n",
        "Fortunately, `sklearn` provides an easy way to split data into train and test sets, using `train_test_split()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZf29Os5b_di"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Let 30% of the data to be a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgtHuRiPb_dp"
      },
      "source": [
        "In the above, I used `random_state=0` when splitting the data. This ensures that anyone that runs the code gets the exact same split. This is good for learning, since we can eliminate things varying due to random splitting of the data, but **when doing this \"for real\", we would not want to set `random_state`**.\n",
        "\n",
        "\n",
        "Now we'll learn an SVM model, using only the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8JaPiFOb_dq"
      },
      "source": [
        "from sklearn import svm\n",
        "titanic_svc = svm.SVC(gamma='auto', random_state=0) # SVC covers not only linear kernel as LinearSVC but also nonlinear kernels\n",
        "titanic_svc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGorV5GZb_dx"
      },
      "source": [
        "We can then get the predicted labels for the test set using the model's `predict()` method and then test the accuracy (out of sample performance):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWADQ9Vub_d0"
      },
      "source": [
        "y_pred = titanic_svc.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDTYKI_zb_d6"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print('{:.2%}\\n'.format(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faSBt761afjb"
      },
      "source": [
        "SO the model performs **okay** (50% accuracy would be achieved with random prediction)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\">Exercise: Titanic SVM - Repeat with random test/train split</font>\n",
        "\n",
        "\n",
        "Repeat the above procedure, but get rid of `random_state=0` (you can do so in both the train/test split AND in the SVC instantiation).  In other words, split the data randomly into 70% training / 30% testing data. Train the SVC model on the training data. Then, evaluate its performance on the holdout test data. Put all of your code into a single code cell so that you can run it multiple times quickly. Notice how the accuracy changes each time you run it, due to the model being trained on different portions of the data.\n"
      ],
      "metadata": {
        "id": "3DANLSD42NN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it out\n",
        "#1. Split the titanic data randomly into 70% train / 30% test (make sure its truly random, so do NOT include random_state keyword argument)\n",
        "\n",
        "#2. Initiliaze the SVC model and fit it (train it) on the training data\n",
        "\n",
        "#3. Calculate the model's accuracy on the holdout testing data and print it out\n",
        "\n",
        "# Run the above multiple times to see how accuracy of the trained model varies with random variation in data train/test split"
      ],
      "metadata": {
        "id": "ichFXgUl2NN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "ZLesO2bc2NN7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_1ae1tob_eD"
      },
      "source": [
        "## Clustering : K-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hBZWTobb_eF"
      },
      "source": [
        "What if we have data where no labels are given ? \n",
        "\n",
        "One class of ML models discovers finds clusters in the data based on patterns. This process is called **unsupervised learning**. [K-means clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) is one type of popular unsupervised learning methods.\n",
        "\n",
        "Here's the basic idea: Assume that data points belong to one of $K$ different clusters. Our task is to find $K$ different centroids (each is the center of the cluster) and assign datapoints to one of these $K$ clusters, such that the  within-cluster variances are minimized.\n",
        "\n",
        "Let $(x_1, x_2, ..., x_n)$ be observations and $(\\mu_1, \\mu_2, ..., \\mu_m)$ be centroids of points in cluster $i, C_i$.\n",
        "\n",
        "Then, our task is to minimize:\n",
        "\n",
        " $\\sum_{i=1}^{m}\\sum_{x\\in C_{i}}\\|x-\\mu_i\\|^2$.\n",
        "\n",
        "This is an example of a model with a hyperparameter, because <font color=red> **we have to specify the number of clusters $K$ at the beginning**</font>\n",
        "\n",
        "Just as before, we'll start by generating synthetic data by drawing datapoints from different distributions. We'll use the exact same data as in the SVM example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWAnF8gJb_eH"
      },
      "source": [
        "# The same data that are generated in the SVM example\n",
        "np.random.seed(1)\n",
        "dat1 = np.random.multivariate_normal(mean=[1,1], cov=[[0.3, 0], [0, 0.3]], size=50)\n",
        "dat2 = np.random.multivariate_normal(mean=[2,1.5], cov=[[0.3, 0], [0, 0.3]], size=50)\n",
        "dat = np.concatenate((dat1, dat2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdbPoXy3b_eM"
      },
      "source": [
        "Assume that we do not know underlying clusters to which data points belong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNoJhDw-b_eN"
      },
      "source": [
        "plt.scatter(dat[:,0], dat[:,1], color='gray'); # but it is originally generated by two different distributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O-z7tXIcQaw"
      },
      "source": [
        "We'll train a model with $K=2$ centroids (we *know* that the data is generated from two distributions in this example, but in a real-world scenario we would not know this, because it is unsupervised learning. In other words, in our training data, we don't the label (which cluster they belong to) of the data points so we can't supervise the model by telling it to get these labels correct.\n",
        "\n",
        "We can specify $K=2$ by setting the keyword argument `nclusters=2` when we instantiate the `KMeans` model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX3ZYyAlb_eZ"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=0) \n",
        "kmeans.fit(dat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5Ckk3VWb_eh"
      },
      "source": [
        "plt.figure(figsize=(10,4));\n",
        "plt.subplot(1,2,1);\n",
        "plt.scatter(dat1[:,0], dat1[:,1]);\n",
        "plt.scatter(dat2[:,0], dat2[:,1]);\n",
        "plt.title('Original data');\n",
        "\n",
        "plt.subplot(1,2,2);\n",
        "labels = kmeans.labels_\n",
        "plt.scatter(dat[:,0], dat[:,1], color=['tab:orange' if x==0 else 'tab:blue' for x in labels]);\n",
        "plt.title('K-means clustering: K=2');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l23yzuOBb_en"
      },
      "source": [
        "You can see that K-means clustering did a fairly good job at detecting the clusters that we created synthetically. Of course, since this is unsupervised learning, we would not know the \"true\" clusters or classes that the data belonged to -- so we can't assess the performance directly.\n",
        "\n",
        "\n",
        "We would also have no way of knowing whether the data is best characterized by two clusters or more. So let's have a look at other values of $K$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\">Exercise: Train a Kmeans model with $K=4$ on the same data</font>\n",
        "\n",
        "Repeat the above steps, but modify the model so that we are using $K=4$. We know that the data for this example were created from 2 clusters (not 4), but in a real-world example, we would not know this. That means we would have to experiment with different values of $K$ and try to find the model that fits our data the best. One way to do this would be to hold out even more data from training, creating a validation subset of the data. We wcould then train the model on the training data for different values of $K$, use the validation subset to pick the model (the value of $K$) that performs the best, and then finally evaluate the performance of that model on the test data.\n",
        "\n",
        "For now, just train the model with $K=4$\n",
        "\n"
      ],
      "metadata": {
        "id": "TtST55zpa8fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it out, as before call your model kmeans"
      ],
      "metadata": {
        "id": "4rGWzGYda8fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the below code to plot the original data and the fitted data"
      ],
      "metadata": {
        "id": "Bx3c_DajcVJB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQQWOc4Nb_e0"
      },
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(dat1[:,0], dat1[:,1]);\n",
        "plt.scatter(dat2[:,0], dat2[:,1]);\n",
        "plt.title('Original data');\n",
        "\n",
        "plt.subplot(1,2,2);\n",
        "labels = kmeans.labels_\n",
        "cmap = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
        "plt.scatter(dat[:,0], dat[:,1], color=[cmap[x] for x in labels]);\n",
        "plt.title('K-means clustering: K=4');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "yYNcTYBea8fT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPH7_xJb_fB"
      },
      "source": [
        "### Example: Unsupervised clustering applied to wine data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5izbjLZb_fE"
      },
      "source": [
        "In this data, chemical compositions and types of wines are given. There are three types of wines (i.e., coming from different winemakers). We will investigate the extent to which k-means clustering can recover the wine types.\n",
        "\n",
        "Note that there isn't any correlation between the label kmeans ascribes and the true label of the wine (on one run, a particular type of wine might be represented with cluster labeled by 1, but on another the same approximate cluster might be assigned the label 2). Because of this, we'll need a metric that is not dependent on the actual value of the labels. A good metric is the [Mutual Information Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html) which is defined by:\n",
        "\n",
        " $MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}$\n",
        "\n",
        "where $|U_i|$ is the number of samples in cluster $U_i$ and $|V_j|$ is the number of samples in cluster $V_j$. We'll use the predicted labels and true labels as $U$ and $V$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYChin8sb_fG"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.metrics import mutual_info_score\n",
        "wine = load_wine() \n",
        "X = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "y = wine.target # there are three types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oijMWQ0Wb_fR"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKosMynnb_fY"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oQWzWjxb_ff"
      },
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "kmeans.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_63XiqXob_fl"
      },
      "source": [
        "y_pred = kmeans.predict(X_test)\n",
        "print('{:.2%}\\n'.format(mutual_info_score(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXgTmnmMb_fo"
      },
      "source": [
        "Not bad, but there is room for improvement. How?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja0Npxxyb_fp"
      },
      "source": [
        "## Preprocessing : Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh0UA6vqb_ft"
      },
      "source": [
        "Often you will encounter data where the features have very different distributions (with very different mean and variance for each column). In such a case, you can benefit from **standardization** -- a process of transforming the features so that each column has zero mean and unit variance. Some ML algorithms will suffer if the data on which they are trained is not standardized. To standardize data, we will use the [`sklearn.preprocessing`](https://scikit-learn.org/stable/modules/preprocessing.html) package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBWgddIab_fu"
      },
      "source": [
        "`StandardScaler` calculates mean and standard devaition of a train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge9kAroqb_fv"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler() # calculate mean and standard deviation of train set\n",
        "scaler.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aY3ED_7b_fz"
      },
      "source": [
        "Let's print the means of the different features (columns):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFlWJanhb_f1"
      },
      "source": [
        "scaler.mean_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSQLQr4Xb_f6"
      },
      "source": [
        "`scaler.scale_` returns the standard deviations for each feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Oshs4FUb_f7"
      },
      "source": [
        "scaler.scale_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIDO2KYTb_f-"
      },
      "source": [
        "To standardize the data, we simply call `scaler.transform` on the original values to get the transformed data. Transformed data will have 0 mean and 1 variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5n5eXaSb_gA"
      },
      "source": [
        "X_train_scaled = scaler.transform(X_train) # You can apply the scaler even to test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJuv-qkXb_gD"
      },
      "source": [
        "Let's check means of transformed data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3OyiMbZb_gE"
      },
      "source": [
        "np.mean(X_train_scaled, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QKMgBrbb_gI"
      },
      "source": [
        "Compared to means of the original data, those of the standardized data are close to zero. Now, train a k-mean clustering model with the standardized data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ7bvzYHb_gJ"
      },
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "kmeans.fit(X_train_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5lCdOf5b_gM"
      },
      "source": [
        "X_test_scaled = scaler.transform(X_test)\n",
        "y_pred = kmeans.predict(X_test_scaled)\n",
        "print('{:.2%}\\n'.format(mutual_info_score(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gqZ2uKtb_gO"
      },
      "source": [
        "You can see that standardizing the data has a pretty substantial impact on the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFWjah-Fb_gP"
      },
      "source": [
        "### Pipelines : chaining pre-processors and estimators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD0vpFxmb_gP"
      },
      "source": [
        "We can do all procedures (standardize data and learn a model) at once by taking advantage of the [`sklearn.pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) package!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzvouLKib_gQ"
      },
      "source": [
        "The below code block shows how to combine the different operations, scaling with the `StandardScaler` and running the `KMeans` algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcSfp44tb_gR"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "pipe = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    KMeans(n_clusters=3, random_state=0)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG_FnXyxb_gT"
      },
      "source": [
        "To execute this pipe, following the rules of sklearn, use `.fit` method.\n",
        "```python\n",
        "pipe.fit(X_train, Y_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFhoHS09b_gW"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = load_wine() \n",
        "X = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "Y = wine.target # there are three types\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=0)\n",
        "\n",
        "pipe.fit(X_train, Y_train)\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "Y_pred = kmeans.predict(X_test_scaled)\n",
        "print('{:.2%}\\n'.format(mutual_info_score(Y_test, Y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAJ3WuS4b_ge"
      },
      "source": [
        "And of course we get the same result as before.  Making a pipeline is useful as frequently you'll want to do several transformations of your data as part of model fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzLsakRUb_gf"
      },
      "source": [
        "## Dimensionality reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs-21vkrb_gg"
      },
      "source": [
        "Dimensionality reduction offers several advantages. (https://en.wikipedia.org/wiki/Dimensionality_reduction)\n",
        "* It reduces the time and storage space required.\n",
        "* Removes  multi-collinearity to improve the interpretation of the parameters of the machine learning model.\n",
        "* It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D.\n",
        "* It avoids the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7uv3YvAb_gh"
      },
      "source": [
        "For example, we can reduce 13 dimensions in the wine data into 2 dimensions. This process helps to understand and visualize complicated data. In this section, we will cover a popular dimensionality reduction method: **principal component analysis (PCA)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZbiiqFdb_gi"
      },
      "source": [
        "#### What is PCA?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnvWcA0Tb_gj"
      },
      "source": [
        "[PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) finds linearly uncorrelated variables by combining existing correlated variables. Let's explore the concept with the wine data. In the wine data, `alcohol` and `color_intensity` are correlated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvHMReNUb_gk"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "wine = load_wine() \n",
        "X = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "y = wine.target # there are three types\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "X_train_scaled = scaler.transform(X_train) # You can apply the scaler even to test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJYl8rU5b_go"
      },
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(X_train_scaled[:,0], X_train_scaled[:,-4])\n",
        "plt.xlabel(\"Alcohol (standardized)\")\n",
        "plt.ylabel(\"Color intensity (standardized)\")\n",
        "print(\"Correlation between alchol and color intensity is\", round(X.alcohol.corr(X.color_intensity), 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0ABrRX9b_gs"
      },
      "source": [
        "As the two variables are correlated, significant amount of variances between them can be captured through a new variable. PCA returns this new variable by combining correlated ones. The new variable is represented as the arrow on the below plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8fx4Rkxb_gv"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2, random_state=0) # 13 dimensions to 2 dimensions\n",
        "pca.fit(X_train_scaled[:, (0,-4)]) # find principal components"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K_q_VDYb_g3"
      },
      "source": [
        "plt.figure(figsize=(6,6));\n",
        "plt.scatter(X_train_scaled[:,0], X_train_scaled[:,-4], alpha=0.3);\n",
        "plt.xlabel(\"Alcohol (standardized)\");\n",
        "plt.ylabel(\"Color intensity (standardized)\");\n",
        "plt.annotate(\"\", [0,0], -3*pca.explained_variance_ratio_[0]*pca.components_[:,0], \n",
        "             arrowprops=dict(arrowstyle='<-', linewidth=3, color='red'));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koeqZmKCb_g9"
      },
      "source": [
        "In this way, PCA finds a given number of components (`n_components`) that are uncorrelated and explain variance well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-MalDzcb_g-"
      },
      "source": [
        "\"pca.explained_variance_ratio_\" summarizes how much variance that a component explains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-QtCkxob_hA"
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ebLePtb_hC"
      },
      "source": [
        "This shows that the first principal component (red arrow) explains about 78% of the total variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBTMrmXIb_hD"
      },
      "source": [
        "Let's apply PCA for the entire wine data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0ujm6qzb_hE"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2, random_state=0) # 13 dimensions to 2 dimensions\n",
        "pca.fit(X_train_scaled) # find principal components"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-IwUWDhb_hJ"
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VklOxnlb_hO"
      },
      "source": [
        "About 37\\% of the variance is explained by the first principle component and about 19\\% of the variance is explained by the second principle component. It means that the first two components capture more than half of all the variance of the data. \n",
        "\n",
        "So, projecting the wine data onto the first two principal components can give a good overview of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h63-M3a-b_hQ"
      },
      "source": [
        "pca_transformed = pca.transform(X_train_scaled) # project the data onto principal components"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MSsVmNQb_hV"
      },
      "source": [
        "cmap = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
        "plt.scatter(pca_transformed[:,0], pca_transformed[:,1], color = [cmap[x] for x in y_train]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKIvxPPGb_hb"
      },
      "source": [
        "Three wine types are separated well by the first two principal components"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Real-World Data for sklearn models"
      ],
      "metadata": {
        "id": "dZBvKgY0dSNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforming categorical data to dummy variables"
      ],
      "metadata": {
        "id": "RK4FBXq7kfNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost all models in sklearn rely on numerical data (as we have seen with the artifical and real world examples we have looked at so far). But often we have dataframes that contain categorical data. How can we pre-process categorical data so that we can train sklearn models on it?\n",
        "\n",
        "Consider the following dataset from a bank that has granted some applicants loans:"
      ],
      "metadata": {
        "id": "hRcN7SUOdZGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loan_df = pd.read_csv('https://raw.githubusercontent.com/dylanwalker/MGSC496/main/datasets/loan_data_set.csv')\n",
        "loan_df.dropna(inplace=True) # We'll drop columns that have missing values\n",
        "loan_df.drop(columns=['Loan_ID'],inplace=True) # this is a unique identifier and we wouldn't want to let our model train on it, since this is a useless feature for test data \n",
        "loan_df.head()"
      ],
      "metadata": {
        "id": "OeTWVvKNMRH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the columns are numerical, such as `ApplicantIncome` and `Loan_Amount_Term`, while other columns are categorical, such as `Gender` and `Property_Area`. We saw before (in the pandas lecture), that we can use some tricks to convert categorical data to numerical data.\n",
        "\n",
        "For example, we could create column called `Gender_num` by doing:\n",
        "```python\n",
        "loan_df['Gender_num'] = (loan_df.Gender=='Male')*1.0\n",
        "```\n",
        "This would create a numerical column where the value was 1 for rows when `Gender` was `Male` and 0 otherwise. However, this trick would not work if the column conatained more than two values. Another problem wit this approach is that the model we train with the data would not know that numerical values could only be 1 and 0 for this column. A better approach is to create multiple columns from a single categorical column and make dummy variables for each of the values it could take. We can do this using the function [`pd.get_dummies()`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) on the categorical column.\n",
        "\n",
        "For example:"
      ],
      "metadata": {
        "id": "ADweIoKceQK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(loan_df.Property_Area)"
      ],
      "metadata": {
        "id": "FAFwaj6Af5Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that this created 3 dummy variable columns for each of the 3 possible values of `Property_Area` (`Rural`, `Semiurban`, `Urban`).\n",
        "\n",
        "In practice, we would want to hold out one of these categories (one we know that the values for `Semiurban` and `Urban`, we know the value for the third category. We can do this using the keyword argument `drop_first=True`:"
      ],
      "metadata": {
        "id": "ciHIraJegUCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(loan_df.Property_Area, drop_first=True)"
      ],
      "metadata": {
        "id": "gScbstv4g0i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is even possible to apply `pd.get_dummies()` to multiple columns in a dataframe at once. For example: "
      ],
      "metadata": {
        "id": "mxcgYkCBg6ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(loan_df.loc[:,['Gender','Property_Area', 'Education']], drop_first=True)"
      ],
      "metadata": {
        "id": "1QTeZ1kdhBdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact, `pd.get_dummies()` will ignore columns that are numerical, so we could apply it to the entire dataframe in this case:"
      ],
      "metadata": {
        "id": "tIRxYa7fhqAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\">Exercise: Use `pd.get_dummies()`, make `X` and `y` from `loan_df`, and train/test split the data </font>\n",
        "\n",
        "Let's complete all the steps necessary to go from a starting dataframe to data that we will use to train a model in sklearn.\n",
        "\n",
        "<br />\n",
        "\n",
        "Do the following:\n",
        "\n",
        "1. Apply `pd.get_dummies()` to the whole `loan_df` dataframe with `drop_first=True` and look at the results.\n",
        "2. Using the output from the last step, create a variable called `X` that contains all the feature columns (everything except the one that captures whether the loan was granted (`Loan_status_Y`).\n",
        "3. Create a variable called `y` that captures the outcome variable (`Loan_status_Y`)\n",
        "4. Use `train_test_split()` on `X` and `y` to get `Xtrain`, `Xtest`, `ytrain`, `ytest` \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Uu7vVMGPi6ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it out\n",
        "\n",
        "# 1. Use pd.get_dummies() with drop_first=True to transform the dataframe\n",
        "\n",
        "# 2. Create the features variable X\n",
        "\n",
        "# 3. Create the outcome variable y\n",
        "\n",
        "# 4. Use train_test_split to get training and test data for X and y\n"
      ],
      "metadata": {
        "id": "LTYsnebDi6a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look over what you created. Does it make sense? Do you understand what you're doing?"
      ],
      "metadata": {
        "id": "VsmA6wFljJ1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "D2OPXZHsi6a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforming numerical data to categorical data"
      ],
      "metadata": {
        "id": "l4eLICgAknf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes we have numerical columns and we want to transform them to categorical ones (which we may then transform to dummies).\n",
        "\n",
        "To see how numerical data is distributed, we can use the dataframe methods:\n",
        "* [`df.col.describe()](https://pandas.pydata.org/docs/reference/api/pandas.Series.describe.html)` - shows statistics of a column including mean, std, min, and percentile values\n",
        "*[`df.col.value_counts()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html) - shows how many rows take each distinct value\n",
        "\n",
        "Once we understand this, we can bin numerical data into meaningful categories using the pandas functions:\n",
        "* [`pd.cut()`](https://pandas.pydata.org/docs/reference/api/pandas.cut.html) - cut the data into bins, based on the edges of the bins that you specify\n",
        "* [`pd.qcut()`](https://pandas.pydata.org/docs/reference/api/pandas.qcut.html) - cut the data into bins, based on the percentile values that you specify \n",
        "\n",
        "For example, lets look at the column `ApplicantIncome`:"
      ],
      "metadata": {
        "id": "RrdHn-iDkr47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loan_df.ApplicantIncome.describe()"
      ],
      "metadata": {
        "id": "0pN-zd82ljWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the 0%, 25%, and 75% values of ApplicantIncome are all different (that might not always be the case), we can create bins whose edges are these values and bin the rows into categories:"
      ],
      "metadata": {
        "id": "fzb9L0h1mVvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loan_df.ApplicantIncome"
      ],
      "metadata": {
        "id": "Ww9LjntinGZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.qcut(loan_df.ApplicantIncome, q=[0,0.25,0.75,1.0], labels=['low','medium','high']) # The low labelk will be assigned to rows whose values fall between the 0% and 25% value of ApplicantIncome, etc."
      ],
      "metadata": {
        "id": "61rJbm6BmoG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's have a look at the column `Loan_Amount_Term`:"
      ],
      "metadata": {
        "id": "hMtnTGk3nxDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loan_df.Loan_Amount_Term.describe()"
      ],
      "metadata": {
        "id": "GOZIbip-n42P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seens like most of the loans have a term of 360 months and the percentile values are the same for 25%, 50%, 75%. So binning the data by percentiles doesn't really make sense for this variable. Let's have a look at how the distinct values of `Loan_Amount_Term` are distributed:"
      ],
      "metadata": {
        "id": "IspFi6DYn9CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loan_df.Loan_Amount_Term.value_counts()"
      ],
      "metadata": {
        "id": "PhKjtVkxoP9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most popular loan term is 360 months. We might say that such loans are `normal` terms, while loans with shorter terms are categoried as `short`, and loans of longer term are categorized as `long`. We can accomplish this categorization with `pd.cut()`: "
      ],
      "metadata": {
        "id": "tyZocjg8oZ-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loan_term_cat = pd.cut(loan_df.Loan_Amount_Term, bins=[0,359,361,480], labels=['short','normal','long'])\n",
        "loan_term_cat"
      ],
      "metadata": {
        "id": "7GyQn3SWo1th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then see how many loans fall into each of the categories we specified:"
      ],
      "metadata": {
        "id": "5UdHQlTLpkT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loan_term_cat.value_counts()"
      ],
      "metadata": {
        "id": "oicIPW_yphbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, choosing the right categorization requires thinking about the context of the data and making judgment calls. \n",
        "\n",
        "You might be wondering:\n",
        ">**Why should I change numerical columns to categorical ones, just to change it into dummy variables? Why not just use the numerical column we already have?**\n",
        "\n",
        "Here are two reasons:\n",
        "1. Changing a numerical variable to a categorical one reduces variability that might not be meaningful. This can yield results which are easier to interpret.\n",
        "2. Some models in sklearn want dummies instead of columns (see, for example [Categorical Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#categorical-naive-bayes)) "
      ],
      "metadata": {
        "id": "d794zIacp3aj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-XTBQvhy0uX"
      },
      "source": [
        "# A ton of models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LizREaSHy3lL"
      },
      "source": [
        "Sklearn has a ton of models that you can use. Covering them all is well outside of the scope of this class. Though you are likely familiar with many of them  from your prior coursework. In lecture, I will talk about:\n",
        "* K nearest neighbor\n",
        "* Decision trees\n",
        "* Naive bayes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedback\n",
        "What did you think about this notebook? What questions do you have? Were any parts confusing? Write your thoughts in the text box below.\n",
        "\n",
        "<font size =2> note: You can double click this text box in colab to edit it.</font>\n",
        "\n",
        "PUT YOUR THOUGHTS HERE"
      ],
      "metadata": {
        "id": "JOPdib-mkdbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit\n",
        "Don't forget to submit your notebook before class! Make sure you have saved your work (**Colab Menu: File-> Save**) and then download a pure python copy (**Colab Menu: File-> Download -> Download .py**) and a python notebook copy (**Colab Menu: File-> Download -> Download .ipynb**). You will upload both of these to the assignment on the canvas page.\n"
      ],
      "metadata": {
        "id": "-Od5aIDFnSYP"
      }
    }
  ]
}