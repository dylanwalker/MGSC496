{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanwalker/MGSC496/blob/main/MGSC496_R08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Your Info\n",
        "\n",
        "your_name = '' #@param {type:\"string\"}\n",
        "your_email = '' #@param {type:\"string\"}\n",
        "today_date = '' #@param {type:\"date\"}\n"
      ],
      "metadata": {
        "id": "qCErBFYSbdOw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to \"read\" this notebook\n",
        "\n",
        "As you go through this notebook (or any notebook for this class), you will encounter new concepts and python code that implements them -- just like you would see in a textbook. Of course, in a textbook, it's easy to read code and an explanation of what it does and think that you understand it.\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "### Learn by doing\n",
        "But this notebook is different from a textbook because it allows you to not just read the code, but play with it. **You can and should try out changing the code that you see**. In fact, in many places throughout this reading notebook, you will be asked to write your own code to experiment with a concept that was just covered. This is a form of \"active reading\" and the idea behind it is that we really learn by **doing**. \n",
        "<br />\n",
        "<br />\n",
        "\n",
        "### Change everything\n",
        "But don't feel limited to only change code when I prompt you. This notebook is your learning environment and your playground. I encourage you to try changing and running all the code throughout the notebook and even to **add your own notes and new code blocks**. Adding comments to code to explain what you are testing, experimenting with or trying to do is really helpful to understand what you were thinking when you revisit it later. \n",
        "<br />\n",
        "<br />\n",
        "### Make this notebook your own\n",
        "Make this notebook your own. Write your questions and thoughts. At the end of every reading notebook, I will ask the same set of questions to try to elicit your questions, reaction and feedback. When we review the reading notebook in class, I encourage you to   \n",
        "\n"
      ],
      "metadata": {
        "id": "LHv80meh7uAs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZqH0luro3Tv"
      },
      "source": [
        "# Code Preface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEtG0MVbovdk"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49gdNNhso9_S"
      },
      "source": [
        "# Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eL2GnygpB6Y"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1OFx-0HlKzV2kOVaD2SNkZwOUpHhClMiJ)\n",
        "\n",
        "## What is PyTorch?\n",
        "\n",
        "Pytorch is an open source machine learning framework that we can use to build and train artificial neural networks.\n",
        "\n",
        "\n",
        "## Why PyTorch?\n",
        "\n",
        "You might have heard about a very popular Neural Network library called Tensorflow. And you might be wondering \"Why aren't we learning Tensorflow?\".\n",
        "\n",
        "There are a few answers to this question:\n",
        "- Declarative vs Imperative:\n",
        " - Tensorflow was designed to be **declarative**. In Tensorflow, you set up your neural network architecture as a static graph before a model can run. The graph is full of all of these placeholders that will be replaced with tensors built from data when the model is run.  In this sense, the model is kind of an enclosed box that you define ahead of time, with only a few ways that you can communicate or pass data into this box. \n",
        " - In contrast, PyTorch is **imperative**. You can define, change and pass data through nodes of the graph as you go. This means you have the ability to change things on the fly and peer into what is happening, and even **debug** easily.\n",
        "- Static vs Dynamic:\n",
        " - Because you build static graphs in Tensorflow, it is harder to implement dynamic neural network architectures. Other dynamical things, such as input data that has varying size have to be handled with workarounds (such as padding the data).\n",
        " - But PyTorch is naturally dynamic and so its relatively easy to do these things.\n",
        "- Pedagogical reasons:\n",
        " - The Tensorflow API is a bit cluttered. There are many ways to do things and it isn't always clear what is the best way to do it. While there is a ton of support online (because the community of Tensorflow users is quite large), it has evolved significantly over the years and you may easily find outdated methods and approaches.\n",
        " - PyTorch is very easy to learn and is more \"Pythonic\".\n",
        "\n",
        "\n",
        "To be clear, there are lots of other differences and Tensorflow has some advantages. Not to mention that Tensorflow has a new Eager execution framework that allows you to do more dynamic things with it. Its also very easy to deploy in production and can be computationally efficient. Ultimately if you continue to learn and work with neural networks, you will likely have to learn both frameworks. However, I chose PyTorch as a starting framework because I believe it provides a smoother learning curve.  And once you know PyTorch and the fundamentals of neural networks, it will be much easier to learn Tensorflow if you choose. Typically, researchers who are working on creating new and interesting neural network architectures prefer to work in PyTorch.\n",
        "\n",
        "\n",
        "## What does PyTorch do for us?\n",
        "\n",
        "One of the many thing that pytorch provides is the ability to make and work with tensor objects.\n",
        "\n",
        "A tensor is a number, vector, matrix, or any n-dimensional array.  You might be thinking \"Hey, wait a minute, we already have a library for working with numbers, vectors, matrices and n-dimensional arrays -- its called numpy!\".\n",
        "\n",
        "\n",
        "That's true, but pytorch is different because the tensor objects are built to automatically compute gradients (derivatives) when they are linked to one another through an expression. Computing gradients is an important aspect of the backward propagation step that is used in the training loop to train a neural network.\n",
        "\n",
        "We will see how all of this works together, but for now let's just explore how to create and work with some very basic tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQDb-eIYudT7"
      },
      "source": [
        "# Basics of Pytorch tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbqfGqMhKrYk"
      },
      "source": [
        "The first thing we need to do is to import the pytorch module and decide whether we want to execute our code on a CPU (Central Processing Unit) or a GPU (Graphics Processing Unit).\n",
        "\n",
        "The operations involved in working with tensors and training neural network can benefit from parellelization significantly. Just as tools such as numpy take advantage of linear algebra libraries to vectorize operations, pytorch can also take advantage of libraries to parallelize computations. Whereas modern CPUs have increased the number of cores over the years, they still pale in comparison to the number of cores on GPUs which can number in the hundreds to thousands. Neural networking frameworks such as Pytorch can leverage libraries, such as CUDA (a parallelel computing framework developed by NVIDIA) to take advantage of all the GPU cores.\n",
        "\n",
        "Of course, in order to do this, we need to be executing the code on a machine that has a GPU.\n",
        "\n",
        "Google Colab offers GPUs and TPUs (Tensor Processing Units) as options under the runtime settings: \n",
        "`Runtime->Change Runtime Type->Hardware Accelerator-> GPU or TPU`. \n",
        "\n",
        "Let's do this now, so that we have access to a GPU.\n",
        "\n",
        "We won't need to do this for now, as we'll be executing relatively shallow neural nets that can still be trained in a reasonable amount of time with just a CPU.\n",
        "\n",
        "\n",
        "When you work with pytorch, you can specify whether to use the CPU or GPU (if one is available). So I will show you code to do this.\n",
        "\n",
        "NOTE: When first building networks, its often better to work on the CPU, as debugging is much easier. Converting your tensors and model to run on the GPU is relatively easy and can be done at the last step when you want to train it over many epochs (iterations over the data).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Cl09mYVmG1"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available() # This will return True if we have  setup a GPU on this Colab runtime or False otherwise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQTE9Y7aV3XS"
      },
      "source": [
        "Now let's see how to make some simple tensors. It is very similar to how you create multi-dimensional arrays in numpy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VatKhQn_V6_J"
      },
      "source": [
        "x = torch.tensor([[0.,3.0,-3.4],[-2.4,8.0,5.9]]) # A 2x2 tensor of floats\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00wdD4PzWOBp"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUo3vjE6WdlA"
      },
      "source": [
        "x.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOfNUjByWjWk"
      },
      "source": [
        "torch.zeros(2,2) # a 2x2 tensor with all values set to 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrfbf8kDWtTt"
      },
      "source": [
        "torch.rand(1) # a 1-D tensor (scalar) filled with random values."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7pInScZXc36"
      },
      "source": [
        "You can do all the things with tensors that you can do with numpy, such as the usual arithmetic operations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bszszMvZW2n4"
      },
      "source": [
        "x = torch.ones(3, 5)\n",
        "y = torch.rand(3, 5)\n",
        "print(x)\n",
        "print(y)\n",
        "print(x+y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n05nEqJfXu1K"
      },
      "source": [
        "y/x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppargG41X5KX"
      },
      "source": [
        "And tensors in torch have their own version of Ufuncs and support broadcasting (just like numpy):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PvQMOqDX_eC"
      },
      "source": [
        "y+1 # will add 1 to every element of y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_DUq-6OYObR"
      },
      "source": [
        "Tensors can live in the main memory of the machine (if we are going to work with them on the CPU) or they can live in the graphics cards dedicated memory (if we are going to work with them on the GPU):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de6Zyp0oYYV5"
      },
      "source": [
        "some_cpu_tensor = torch.rand(5,4)\n",
        "some_cpu_tensor.device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrmj44eNYgXE"
      },
      "source": [
        "some_gpu_tensor = torch.cuda.FloatTensor(5,4).uniform_() # notice the call to do this on the GPU is a bit different\n",
        "some_gpu_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBZZI1zBZt7N"
      },
      "source": [
        "Notice the property `device` indicates `cuda` (i.e., that it lives on the GPU)\n",
        "\n",
        "We can also move a tensor to the GPU or CPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkdJHdT-Zx5_"
      },
      "source": [
        "some_cpu_tensor.to(\"cuda\") # This will return a tensor that is identical to the one we created earlier, but that lives in the memory on the GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0vnL9mLZ8Tx"
      },
      "source": [
        "some_gpu_tensor.to(\"cpu\") # This will return a tensor that is identical to the one we created earlier, but the lives in the memory on the CPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRJqhnpsaj2p"
      },
      "source": [
        "Note that the above calls return copies of the tensor that live in the memory of either the CPU or GPU, but they do not affect the original tensor we created. If we want to work with that copy we have to assign it to a variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnjfMg22agRo"
      },
      "source": [
        "some_cpu_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2gLV568aw_F"
      },
      "source": [
        "`some_cpu_tensor` still lives in the CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg2CblYga4dI"
      },
      "source": [
        "another_gpu_tensor = some_cpu_tensor.to(\"cuda\")\n",
        "another_gpu_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnRPmODVbE7x"
      },
      "source": [
        "Pytorch tensors support all the usual numpy functions. So you should feel right at home working with them. For that reason, I won't go through all the basic things that you can do with tensors. \n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxQGhmO2bKwg"
      },
      "source": [
        "some_gpu_tensor.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pJ-0PLZbeqM"
      },
      "source": [
        "# Gradients and Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6qiwJ3FblST"
      },
      "source": [
        "At this point, you might again be wondering why we need pytorch if numpy can do all these things already. But there is one very important thing that pytorch tensors do that is very useful for working with neural networks. They can automatically differentiate -- or in other words, they can calculate gradients (multi-dimensional derivatives) when we combine tensors together with mathematical expressions.  This is exactly what is needed for the \"back propagation\" step of training a neural net.\n",
        "\n",
        "When we create a tensor, we can tell pytorch to enable calculation of  gradients for that tensor by specifying the keyword argument `requires_grad=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjDMrlcLc3Nk"
      },
      "source": [
        "x = torch.rand(1,requires_grad=True)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zZkwwxqg5Uj"
      },
      "source": [
        "`requires_grad` is contagious -- any expression that depends on a tensor that requires a gradient will also require a gradient:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNcDzWnAdJLz"
      },
      "source": [
        "y = x**2 # remember ** means \"raise to the power\"\n",
        "print(y)\n",
        "print(y.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZEslSz4hDj4"
      },
      "source": [
        "Two things to notice:\n",
        "1. `y` has a function associated with it called `grad_fn`. The name of the functions gives a hint at what it is for `PowBackward0`. The `Pow` indicates that it is from a power operation and `Backward` indicates that it is used to propagate the gradient backward. \n",
        "2. Because `y` depends on `x` which requires a gradient, it also requires a gradient (contagious)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Da3Wd7vhRN0"
      },
      "source": [
        "Whenever we have a **scalar tensor** (a tensor that holds only one value, not a vector or multi-dimensional array) that depends on other tensors, we can tell pytorch to calculate the gradient. We do this by calling `y.backward()`. The term backward here refers to back propagation and it is the fundamental way that we train a neural network. By \"train\" I mean, adjust the weights and biases of a NN to minimize the loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3L04U_7iMoT"
      },
      "source": [
        "y.backward() # tell pytorch to calculate the gradients involved in the definition of y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4jXj4hWdS3t"
      },
      "source": [
        "print(f'dy/dx = {x.grad}')\n",
        "print(2*x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBbqvULTikDe"
      },
      "source": [
        "Notice that pytorch correctly calculated the gradient:  $dy/dx=2x$\n",
        "\n",
        "How this works is that pytorch takes expressions between tensors and uses them to build a *computational graph* behind the scenes. All operators in this graph are implemented by the **autograd** package. Pytorch can pass data forward through this graph (i.e., start with the input, perform the operations to get the output) and also propagate things backward through this graph by applying the chain rule (remember your calculus?). Autograd is designed in a modular way so that the functions it implements only have to worry about their own role (their own differentiation with respect to inputs and outputs) in the chain rule process.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917zUBzHi24w"
      },
      "source": [
        "Some important things to note here:\n",
        "- Automatic differentiation is all handled for us in the background by autograd -- we won't actually need to manually look at `.grad`'s\n",
        "- By default, the `.grad` values accumulate. This is useful for pytorch do \"do its thing\" when we tell it to backward propagate the loss. But if we wanted to handle some portions of this process manually, we have to remember that gradients will accumulate unless we set them to zero. \n",
        " - We can zero gradients of some tensor `x` with a call to `x.grad.zero_()`\n",
        " - we'll actually do this today when we manually implement linear regression.\n",
        " - More generally, we might want to zero the gradients of all the tensors in a neural network. We'll talk about this later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NABzneJ5okX"
      },
      "source": [
        "x.grad.zero_()\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\">Exercise:  Create and play with tensors</font>\n",
        "\n",
        "1. Create three tensors, $a$, $b$ and $c$, each of which is a 1D random scalar.\n",
        "2. Compute a new scalar tensor $d = 2a^2 + 3b^3 + 4c$\n",
        "3. Since $d$ is a scalar we can tell pytorch to calculate the gradients involved in its calculate. Do so. Now print out the gradients of $a$, $b$, and $c$. Do they make sense? Why?\n",
        "\n"
      ],
      "metadata": {
        "id": "IpDtp1rvHdNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it out\n"
      ],
      "metadata": {
        "id": "Qsh-jBDT5Wc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The way the autograd part of pytorch is designed, it only calculates gradient values for \"leaf nodes\", that is for tensors that do not depend on other tensors. This will be true for all parameters (weights and biases) of a neural network (which are the only things we need the gradients for - to figure  out which direction and how much to nudge them during training). "
      ],
      "metadata": {
        "id": "VDqPoWf8Q1L-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "PgNbgx-R1Yr0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isbr0Jv65nO9"
      },
      "source": [
        "\n",
        " ## How does autograd do it's thing?\n",
        " * At this stage, you don't need to know... and you probably don't want to know.\n",
        " - Ok, really want to know? Have a [look at this video](https://youtu.be/MswxJw-8PvE) or [this blog post](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/) or [this one](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95).\n",
        " - Autograd is a really clever implementation of automatic differentiation. So, naturally it is not trivial to understand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Moving data between types: Scalars, Numpy Arrays, and Tensors"
      ],
      "metadata": {
        "id": "wdvK2OTr-aLa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XwgehineanS"
      },
      "source": [
        "<font size=5> Getting a scalar or a numpy array from a tensor</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMmlgIjael7H"
      },
      "source": [
        "There are some circumstances where you might want to get the value of a scalar rather than as a tensor type.  You can do this with the `.item()` method:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kx_TfI7exdN"
      },
      "source": [
        "st = torch.tensor([5.2])\n",
        "st.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWCSB21GfDEF"
      },
      "source": [
        "In some circumstances, we might want to get a tensor as a numpy array.  If we want to do this, we must:\n",
        "-  use the `.detach()` method to get rid of any gradient part of the tensor and keep only the array part\n",
        "- then we can use the `.numpy()` method to get the detached tensor as a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGQf5qzTfgfH"
      },
      "source": [
        "at = torch.rand(10, requires_grad=True)\n",
        "at.detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=5>Getting a tensor from a numpy array</font>"
      ],
      "metadata": {
        "id": "UoxYz9zx-6XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can just call `torch.tensor()` on a numpy array to turn it into a tensor:"
      ],
      "metadata": {
        "id": "VuhwHY9w_Mw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "some_np_array = np.random.rand(10)\n",
        "torch.tensor(some_np_array)"
      ],
      "metadata": {
        "id": "mNCwCMal_UEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=5>Getting a tensor from a dataframe</font>"
      ],
      "metadata": {
        "id": "_IdWzkeU9lya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show how to define tensors when starting with a dataframe, we'll use the example from last reading/lecture --  here's the scaled data from our \"is it a cat\" prediciton problem:"
      ],
      "metadata": {
        "id": "TxuNF_7t94L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code to load the scaled cat_df\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "cat_df = pd.DataFrame({'is_mammal':[1,1,1,0],\n",
        "                       'four_legged':[0,1,1,1],\n",
        "                       'body_weight_lbs':[70.0,8800.0,9.0,0.5],\n",
        "                       'body_height_inches':[36.0,324.0,9.0,1.0],\n",
        "                       'has_thumbs':[1,0,0,0],\n",
        "                       'animal':['chimp','elephant','cat','lizard'],\n",
        "                       'is_cat':[0,0,1,0]})\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(cat_df.loc[:,'is_mammal':'has_thumbs'])\n",
        "cat_df.loc[:,'is_mammal':'has_thumbs'] = scaler.transform(cat_df.loc[:,'is_mammal':'has_thumbs'])\n",
        "cat_df.head()"
      ],
      "metadata": {
        "id": "ZrNOsG6r9vLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall, we wanted to use the columns `is_mammal` to `has_thumbs` as the features $X$ and the column`is_cat` as the outcome $y$. \n",
        "\n",
        "Remember that pandas uses numpy on the backend (for now... [this is changing](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i)), so we can get the numpy array from any dataframe or subset of its rows and columns, with the attribute `.values`. \n",
        "\n",
        "For example:"
      ],
      "metadata": {
        "id": "YGQoCjtzRGBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df.loc[:,'is_mammal':'has_thumbs'].values # Using .values gives us a numpy array that we can pass to torch.tensor()"
      ],
      "metadata": {
        "id": "Xc8FFZnqTucF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can just call `torch.tensor()` on these numpy arrays to define our features (input) $X$ and outcome (output) $y$. We can accomplish this in a single line of code, like this:"
      ],
      "metadata": {
        "id": "RqoUvtf-UyqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(cat_df.loc[:,'is_mammal':'has_thumbs'].values)\n",
        "X"
      ],
      "metadata": {
        "id": "ZjkJrOg4RX7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor(cat_df.loc[:,'is_cat'].values)\n",
        "y"
      ],
      "metadata": {
        "id": "yCKmRGiPRx8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Single Neuron with Pytorch"
      ],
      "metadata": {
        "id": "jKJBjnfRocCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example of implementing the NN training loop that we talked about in class.   "
      ],
      "metadata": {
        "id": "PNEPT45QokZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to train single neuron \"is_cat\" predictor\n",
        "\n",
        "\n",
        "# Load the data and scale it \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "cat_df = pd.DataFrame({'is_mammal':[1,1,1,0],\n",
        "                       'four_legged':[0,1,1,1],\n",
        "                       'body_weight_lbs':[70.0,8800.0,9.0,0.5],\n",
        "                       'body_height_inches':[36.0,324.0,9.0,1.0],\n",
        "                       'has_thumbs':[1,0,0,0],\n",
        "                       'animal':['chimp','elephant','cat','lizard'],\n",
        "                       'is_cat':[0,0,1,0]})\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(cat_df.loc[:,'is_mammal':'has_thumbs'])\n",
        "cat_df.loc[:,'is_mammal':'has_thumbs'] = scaler.transform(cat_df.loc[:,'is_mammal':'has_thumbs'])\n",
        "\n",
        "\n",
        "# Make the features (inputs), X,  and outcomes (targets), y tensors from the dataframe \n",
        "X = torch.tensor(cat_df.loc[:,'is_mammal':'has_thumbs'].values)\n",
        "y = torch.tensor(cat_df.loc[:,'is_cat'].values)\n",
        "\n",
        "# Instead of returning the prediciton true or false for each y, we'll return the sigmoid.\n",
        "# We can think of this as a score that reflects how much the NN thinks each row is a cat\n",
        "# This will be more useful in calculating the cost function\n",
        "def sigmoid(x):\n",
        "  sigmoid = 1/(1+torch.exp(-x))\n",
        "  return sigmoid\n",
        "\n",
        "# Initialize the weights/bias to random values\n",
        "w = torch.tensor(np.random.randn(5),requires_grad=True)\n",
        "b = torch.tensor(np.random.randn(1),requires_grad=True)\n",
        "\n",
        "for epoch in range(1,1000): # run over the entire training data 1000 time\n",
        "  yhat = sigmoid(X @ w + b) # calculate the predicted value. @ means matrix multiplication, as we discussed in class (before with numpy, we used np.dot(), but this is more general)\n",
        "  C = ((yhat - y)**2).sum() # Calculate the cost -- a single number representing how badly our NN did across all the training data\n",
        "  if epoch%100==0: # Every 100 epochs, we will print out the predicted values and the cost function\n",
        "    print(f\"yhat = {yhat}\")\n",
        "    print(f\"cost: {C}\")\n",
        "  C.backward() # Tell pytorch to perform backward propagation, to get the gradients for w and b\n",
        "  with torch.no_grad(): # This line tells pytorch not to modify the computation graph for gradients for anything in the indented block below \n",
        "    w-=w.grad*5e-2 # Update the weights by taking a tiny step in the right direction\n",
        "    b-=b.grad*5e-2 # Update the bias by taking a tiny step in the right direction\n",
        "    w.grad.zero_() # set the gradient values in w and b to zero, so we can calculate them fresh on the next epoch\n",
        "    b.grad.zero_()\n",
        "\n"
      ],
      "metadata": {
        "id": "EjFJyDIyk-WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some things to note about the above example:\n",
        "* When we did this with just numpy, we just repeatedly tried different random values of the weights/bias until we got the right prediction\n",
        "* With pytorch, we can calculate the gradients, so can use them to adjust the weights a tiny bit in the right direction. We used a constant learning rate of 0.05\n",
        "*Notice how the cost is dropping with every epoch -- this is good! The NN is \"learning\"\n",
        "* The final prediction is pretty good. It think the cat row is a cat with a score of $> 0.9$ and the others with a score of $< 0.06$\n",
        "* We used `yhat = sigmoid(X @ w + b)` which is equivalent to $\\hat{y}_i =  \\delta ( w_0X_{i0}+w_1X_{i1}+w_2X_{i2}+w_3X_{i3}+w_4X_{i4}+b )$. This is a more general form, which would also work if we had more than one outcome value (we will in the next example)."
      ],
      "metadata": {
        "id": "uK9OZlOZgXgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\">Exercise:  Build a single neuron trained to predict wine quality </font>\n",
        "\n",
        "Following the same structure as the above example, train a single neuron to predict the quality of red wine based on many different features. Run the code below to read in the dataset, then:\n",
        "\n",
        "<br />\n",
        "\n",
        "1. Like in the example above, use the sklearn standard scalar to scale all the feature columns. Rescale the target (outcome), which is a number from 1 to 10 representing wine quality, to be a number between 0 and 1 (this is what we want if we're going to output a sigmoid for $\\hat{y}$, because sigmoids can only be between 0 and 1) \n",
        "\n",
        "2. Make tensors for features $X$ and target $y$. (make sure their dtypes match that of the weights and biases, which may be torch.float32)\n",
        "\n",
        "3. Create weight and bias tensors initialized to random numbers. You'll have to figure out what size they should be.\n",
        "\n",
        "4. You can define the same sigmoid function as above to be your \"activation\" function.\n",
        "\n",
        "5. Write code for your training loop. It should do the following:\n",
        " * Compute `yhat`\n",
        " * Calculate the cost function `C` (you can use the same on as in our \"is cat\" example )\n",
        " * Print out the cost every 1000 loops (use an `if epoch%1000==0` codeblock to do this)\n",
        " * Call `C.backward()`\n",
        " * Use a `with torch.no_grad()` codeblock to update the weight/bias tensors, then zero the gradients for both\n",
        "6. Calculate the prediction for the training data using the fully trained model (i.e., get yhat as you did in the training loop, but now with the finalized weights/biases)\n",
        "7. Run the plotting code provided below to plot the performance on the training data \n"
      ],
      "metadata": {
        "id": "HwBiyHHiUvf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine_df = pd.read_csv('https://raw.githubusercontent.com/dylanwalker/MGSC496/main/datasets/winequality-red.csv')\n",
        "wine_df.head()"
      ],
      "metadata": {
        "id": "seLfatPzUvf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it out\n",
        "\n",
        "# 1. Use the sklearn standard scalar to scale all feature columns. Also scale the target column appropriately so it is between 0 and 1.\n",
        "\n",
        "\n",
        "# 2. Make tensors for features  X  and target  y (make sure their dtypes match that of the weights and biases, which may be torch.float32).\n",
        "\n",
        "\n",
        "# 3. Create weight and bias tensors initialized to random numbers. You'll have to figure out what size they should be.\n",
        "\n",
        "# 4. You can define the same sigmoid function as above to be your \"activation\" function. Define it below\n",
        "\n",
        "\n",
        "# 5. Write code for your training loop to train over 20,000 epochs. It should do the following:\n",
        "#    - compute yhat\n",
        "#    - Calculate the cost function C (you can use the same one as in our \"is cat\" example )\n",
        "#    - print out the cost every 1000 loops (use an \"if epoch%1000==0\" codeblock to do this)\n",
        "#    - call C.backward()\n",
        "#    - use a \"with torch.no_grad()\" codeblock to update the weight/bias tensors with a learning rate of 1e-5, then zero the gradients of both\n",
        "\n",
        "\n",
        "# 6. Calculate the prediction for the training data using the fully trained model (i.e., get yhat as you did in the training loop, but now with the finalized weights/biases)\n",
        "\n"
      ],
      "metadata": {
        "id": "gZWHwRZkUvf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code to plot the predictions vs the actual targets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(y,yhat.detach().numpy(),'.',alpha=0.2);\n",
        "plt.xlabel('actual wine quality');\n",
        "plt.ylabel('predicted wine quality');\n"
      ],
      "metadata": {
        "id": "6E_8jQh_Sa49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "50tFixXRUvf9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFs0Rp8yKsMh"
      },
      "source": [
        "# Linear Regression with Pytorch -- the manual way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_CMNQsIugOj"
      },
      "source": [
        "Now we will illustrate how pytorch can be used to implement the main training loop in training a neural network: \n",
        "- forward computation to get the result and calculate the loss by comparing the result to the \"right answer\"\n",
        "- backward propagation to adjust the weights to minimize the loss\n",
        "\n",
        "We'll do this through a simple example of linear regression.\n",
        "\n",
        "The data that we'll work with is data on Apple and Orange crop yields from different geographic regions and average data on temperature, rainfall and humidity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRz42bf1AP2m"
      },
      "source": [
        "import pandas as pd\n",
        "crop_file = 'https://raw.githubusercontent.com/dylanwalker/MGSC496/main/datasets/crop_yield.csv'\n",
        "crop_df = pd.read_csv(crop_file)\n",
        "crop_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(crop_df.loc[:,'Temp_F':'Humidity_pct'].values, dtype=torch.float32) # notice here that we explicitly made the data type a float32\n",
        "inputs"
      ],
      "metadata": {
        "id": "IJgy9FTU85sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXrhlRtUErqB"
      },
      "source": [
        "targets = torch.tensor(crop_df.loc[:,'Apples_ton':'Oranges_ton'].values, dtype=torch.float32) # notice here that we explicitly made the data type a float32\n",
        "targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKa92eiIFTOs"
      },
      "source": [
        "We want to make a linear \"neural network\" that relates the inputs to the targets. In other words we want to implement the equation:\n",
        "\n",
        "\n",
        "$$\n",
        "\\hspace{1cm} Y\\hspace{1cm}=\\hspace{1.cm}X \\hspace{2.1cm} \\times \\hspace{1.cm} W^T \\hspace{1.cm}  + \\hspace{1cm} b \\hspace{1cm}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\left[ \\begin{array}{cc}\n",
        "y_{11} & y_{21} \\\\\n",
        "y_{12} & y_{22} \\\\\n",
        "y_{13} & y_{23} \\\\\n",
        "\\end{array} \\right]\n",
        "%\n",
        "=\n",
        "%\n",
        "\\left[ \\begin{array}{cc}\n",
        "73 & 67 & 43 \\\\\n",
        "91 & 88 & 64 \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "69 & 96 & 70\n",
        "\\end{array} \\right]\n",
        "%\n",
        "\\times\n",
        "%\n",
        "\\left[ \\begin{array}{cc}\n",
        "w_{11} & w_{21} \\\\\n",
        "w_{12} & w_{22} \\\\\n",
        "w_{13} & w_{23}\n",
        "\\end{array} \\right]\n",
        "%\n",
        "+\n",
        "%\n",
        "\\left[ \\begin{array}{cc}\n",
        "b_{1} & b_{2} \\\\\n",
        "b_{1} & b_{2} \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "b_{1} & b_{2} \\\\\n",
        "\\end{array} \\right]\n",
        "$$\n",
        "\n",
        "* Notice that in this example, we have two outcomes (targets) we are trying to predict, the yield of apples and the yield of oranges. Each outcome (target) will weight each of the features and have its own bias, and we have three features (temp, rainfall, humidity), so we have 6 weights and 2 biases.\n",
        "* Also notice that since we are doing linear regression, we don't have a nonlinear activation function at all.\n",
        "\n",
        "We'll start with some random weights and biases -- this is typically how we \"initialize\" the parameters of a neural network -- set them to some random values to start:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HrHZgWuIBGp"
      },
      "source": [
        "w = torch.randn(2, 3, requires_grad=True)\n",
        "b = torch.randn(2, requires_grad=True)\n",
        "print(w)\n",
        "print(b)\n",
        "print(w.dtype) # the dtype of the weights and biases has to match the dtype of the inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReuGkRewIvjl"
      },
      "source": [
        "We'll define our model according to the above equation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGSkN6-LFSi9"
      },
      "source": [
        "# Define the model\n",
        "def model(x):\n",
        "    return x @ w.t() + b  #note that @ is the matrix multiplication operator in numpy or pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E_mX_qkJKPI"
      },
      "source": [
        "We took advantage of broadcasting in the above (since we only have two biases). We also took the transpose of the weight matrix with `.t()` (so that the dimensions match to permit a matrix multiplication). \n",
        "\n",
        "Our model is ready to generate predictions -- although, they won't be very good ones yet because we haven't trained it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MW3JX1HIr5U"
      },
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFTn4XsVJp8X"
      },
      "source": [
        "# Compare with targets\n",
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gf19qVaJxC-"
      },
      "source": [
        "The next step is to define a loss (cost) function, that describes how (poorly) our predictions match the targets. We'll use the mean squared error, since we are implementing linear regression. Different circumstances would require a potentially different loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTiBBjxiKEZW"
      },
      "source": [
        "# MSE loss\n",
        "def mse(t1, t2):\n",
        "    diff = t1 - t2\n",
        "    return torch.sum(diff * diff) / diff.numel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo-8lLvdKMoJ"
      },
      "source": [
        "# Compute loss\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E_EvUXGKihT"
      },
      "source": [
        "We can now calculate the gradients of the loss with respect to the weights and biases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2vNLsITKbOh"
      },
      "source": [
        "# Compute gradients\n",
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9dwjKznKdhJ"
      },
      "source": [
        "# Gradients for weights\n",
        "print(w)\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J53qPdf6Kg5j"
      },
      "source": [
        "# Gradients for bias\n",
        "print(b)\n",
        "print(b.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdfLohkMK8Rp"
      },
      "source": [
        "We know about derivatives and optimization from calculus:\n",
        "![](https://drive.google.com/uc?id=1ywzITKPARYGqM8eS_ha7OFbD4g0wF7fg)\n",
        "\n",
        "if the gradient is positive: \n",
        "- increasing the variable will increase the loss \n",
        "- decreasing the variable will decrease the loss \n",
        "\n",
        "If the gradient is negative:\n",
        "- increasing the variable will decrease the loss \n",
        "- decreasing the variable will increase the loss \n",
        "\n",
        "\n",
        "For now, we'll zero the gradients that for our `w` and `b` tensors, as we'll want to start the training process with no gradients accumulated:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVsvwqIqOw25"
      },
      "source": [
        "w.grad.zero_()\n",
        "b.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhvTMBU5OlM5"
      },
      "source": [
        "We'll use gradient descent as the rule for adjusting our weights and biases:\n",
        "```\n",
        "w -= w.grad * 1e-5\n",
        "b -= b.grad * 1e-5\n",
        "```\n",
        "This is in accordance with the intuition we developed above. We chose to multiply the gradient by a small number (which adjusts how big of a step we take). This is called the \"learning rate\".\n",
        "\n",
        "Now we are ready to implement our training loop. We will pass over our data several times to keep adjusting the weights and biases.  Each pass is called an *epoch*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzEPmrksQfo1"
      },
      "source": [
        "# Here I repeated all of the code to load, prepare the data, define the loss and model functions, for ease of running everything together\n",
        "\n",
        "# Load the dataframe\n",
        "crop_file = 'https://raw.githubusercontent.com/dylanwalker/MGSC496/main/datasets/crop_yield.csv'\n",
        "crop_df = pd.read_csv(crop_file)\n",
        "\n",
        "# Make inputs (features) and the targets (outcomes) tensors\n",
        "inputs = torch.tensor(crop_df.loc[:,'Temp_F':'Humidity_pct'].values, dtype=torch.float32)\n",
        "targets = torch.tensor(crop_df.loc[:,'Apples_ton':'Oranges_ton'].values, dtype=torch.float32) \n",
        "\n",
        "# init weights/biases\n",
        "w = torch.randn(2, 3, requires_grad=True)\n",
        "b = torch.randn(2, requires_grad=True)\n",
        "\n",
        "# define the loss (cost) function\n",
        "def mse(t1, t2):\n",
        "    diff = t1 - t2\n",
        "    return torch.sum(diff * diff) / diff.numel()\n",
        "\n",
        "# define the model\n",
        "def model(x):\n",
        "    return x @ w.t() + b  \n",
        "\n",
        "# Train for 100 epochs\n",
        "losses = []\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    preds = model(inputs)\n",
        "    loss = mse(preds, targets)\n",
        "    loss.backward()\n",
        "    losses.append(loss.item()) # Here we extract the loss (which is a scalar, and store it in a list, so we can plot loss vs epochs later)\n",
        "    with torch.no_grad(): # this ensures that gradients won't be affected in the codeblock -- we already have the values of the gradient from the loss.backward() call\n",
        "        w -= w.grad * 1e-2 \n",
        "        b -= b.grad * 1e-2\n",
        "        w.grad.zero_() # zero the weight gradient after we adjusted the weight\n",
        "        b.grad.zero_() # zero the bias gradient after we adjusted the bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBOO9dXVRBL1"
      },
      "source": [
        "Ok, let's see how we did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF7Q0TMzR0Mv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses);\n",
        "plt.xlabel('Epoch');\n",
        "plt.ylabel('Loss');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the loss decreases with epochs and slowly flattens out. This indicates that further training over the data will not do much good."
      ],
      "metadata": {
        "id": "0_eDtTMOykuY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOCeTpgPRFrf"
      },
      "source": [
        "# Calculate loss\n",
        "preds = model(inputs)\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym4QgGaqRIgt"
      },
      "source": [
        "# Plot predictions vs targets\n",
        "#  In order to get from tensors to numpy arrays that we can plot, we'll have to:\n",
        "#     - call .detach() to return a copy with just the data and not the gradient\n",
        "#     - call .numpy() to return a numpy array, since matplotlib doesn't know how to plot tensors\n",
        "plt.plot(targets.detach().numpy(),preds.detach().numpy(),'.');\n",
        "print(targets.detach().numpy().flatten())\n",
        "print(preds.detach().numpy().flatten())\n",
        "plt.xlabel('targets');\n",
        "plt.ylabel('predictions');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi01cs9zRXBd"
      },
      "source": [
        "Not too shabby. Of course, the real test would be to apply this NN to unseen test data.\n",
        "\n",
        "Now we've seen how to implement the main training loop in pytorch. But we did almost everything else manually.\n",
        "\n",
        "Let's see what it would look like if we did the same thing but now using pytorch's built-in features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nk0vJF0y4ZY"
      },
      "source": [
        "# Linear Regression with Pytorch -- the right way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nknfhNYtUCG2"
      },
      "source": [
        "Instead of doing everything ourselves, we are going to take advantage of the many tools and methods that pytorch has built into it. This includes:\n",
        "- Dataset and Dataloader objects\n",
        "- Neural network layers\n",
        "- Optimizers\n",
        "- Various predefined loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zqre0MLWEap"
      },
      "source": [
        "## Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfFefAbOWHSo"
      },
      "source": [
        "First we need to import some stuff (we don't *have* to import these into our namespace, since they can be accessed from `torch.`, but it makes the calls shorter):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJA6n90TWH3O"
      },
      "source": [
        "# Import tensor dataset & data loader\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CW7yeqNWhTY"
      },
      "source": [
        "We can use the `TensorDataset` class to handle our data. One of the things this does is it lets us grab a row from the inputs and target as a tuple: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMRJxTZLW3BJ"
      },
      "source": [
        "# Define dataset\n",
        "train_ds = TensorDataset(inputs, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNmU0LAPWLtV"
      },
      "source": [
        "# Now we can get rows:\n",
        "train_ds[0:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsw5-aawXGiJ"
      },
      "source": [
        "Next, we will use pytorch's `DataLoader` which allows can split the data into batches when we train (and even shuffle or resample, if we want). This is useful because in many cases datasets are too big to process completely. Shuffling data when training is almost always a good idea and resampling is useful if our data is imbalanced in some way.\n",
        "\n",
        "We'll define the `batch_size` when create the DataLoader. This is just the number of rows of the data that we will train over before updating the parameters. \n",
        "\n",
        "Note: If we update parameters before passing over the entire dataset, then we refer to each batch as a \"mini-batch\". This is what happens in **Stochastic Gradient Descent**. We could also process in batches and only update after passing through the entire dataset. In this case, these are called \"batches\" and we are using **Gradient Descent**. (one reason to batch when doing Gradient Descent is that the full dataset might not fit into memory all at once). You may see the terms \"batch\" and \"mini-batch\" used interchangeably (and sometimes incorrectly), but these are the technically correct definitions of the term.\n",
        "\n",
        "Here we'll set the batch to be the length of the dataset (so that we only update parameters after processing all of the data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl6gBLaVWN-P"
      },
      "source": [
        "# Define data loader\n",
        "batch_size = len(train_ds)\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtfnr_FjXtkH"
      },
      "source": [
        "#We can see how shuffle works by running over this two times\n",
        "for i in range(0,2):\n",
        "  for xb,yb in train_dl:\n",
        "    print(xb,yb)\n",
        "\n",
        "# Now change the batch size in the prior cell and rerun both to see what's going on\n",
        "# Just be sure to set it back to 5 and rerun this before continuing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGbdQNlvX8gC"
      },
      "source": [
        "next(iter(train_dl)) # this will return 5 (batch_size) rows of inputs and targets after shuffling. \n",
        "# iter() defines an iterator and next() tells python to get the next iteration from it."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5IMsoVfehaT"
      },
      "source": [
        "We'll see why being able to change the order and group the data into different sized batches is important when we talk about the optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0lA3GhgYiMR"
      },
      "source": [
        "## Use a predefined linear layer -- nn.Linear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEiFF2UVYnQK"
      },
      "source": [
        "Instead of:\n",
        "- defining weights and biases tensors manually\n",
        "- defining the linear function that uses weights and biases to relate input to ouput via a matrix multiplication\n",
        "- initializing weights and biases to random values\n",
        "\n",
        "We can just use a layer that already does all that: ``torch.nn.Linear()``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LreLLFFlZIcT"
      },
      "source": [
        "model = torch.nn.Linear(3,2) # 3 input (features), 2 outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtwyb54EZN6h"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy4jIfIAZPMH"
      },
      "source": [
        "Instead of manually implementing Gradient Descent to adjust weights and biases according to the gradient and some learning rate, we can use one of Pytorch's built-in optimizers.\n",
        "\n",
        "Here we'll use the optimizer for <font color=blue>Stochastic Gradient Descent (SGD) </font>, but set it so that we're just doing regular <font color=blue>Gradient Descent</font>. The difference between the two is that in SGD, you adjust your weights and biases after running each batch through the network. It's **stochastic** because the data loader shuffles the data, so the order the data is seen varies from run to run. In pytorch, all of these versions (plus some variants) are implement with `torch.optim.SGD()`. Have a look at the [documentation](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4T-EMI_b8aL"
      },
      "source": [
        "# Define optimizer\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-5) # lr is the learning rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "791rE2g2cGTY"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnB9aKvOcIQc"
      },
      "source": [
        "Instead of using our manually defined `mse()` function, we'll use a predefined one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HjB8d5ScQJy"
      },
      "source": [
        "loss_fn = torch.nn.functional.mse_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0k9v371cZit"
      },
      "source": [
        "So, for example, we could compute the loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_974eJtDccfU"
      },
      "source": [
        "loss = loss_fn(model(inputs),targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqQ-aNX1cxPW"
      },
      "source": [
        "## Define a fit function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZKNEVrkc0ZE"
      },
      "source": [
        "Finally we will define a function to fit our model by putting all of the above pieces together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPWSHhMyc9x3"
      },
      "source": [
        "# Define a utility function to train the model, here with regular gradient descent\n",
        "def fit_gd(num_epochs, model, train_dl, loss_fn, opt):\n",
        "    losses = []\n",
        "    model.train() # put the model into training mode (in case it is already in evaluation mode) - we'll talk about this later.\n",
        "    for epoch in range(num_epochs):\n",
        "      loss_for_epoch = 0\n",
        "      opt.zero_grad() # tell the optimizer to zero the gradient so we start fresh on this epoch\n",
        "      for xb,yb in train_dl: # sample a batch of inputs,targets\n",
        "        pred = model(xb) # get the predictions\n",
        "        loss = loss_fn(pred, yb) # calculate the loss for these predictions\n",
        "        loss_for_epoch += loss.item() # add loss for this batch to the running total of the loss for this epoch\n",
        "        loss.backward() # propagate the loss backward\n",
        "      losses.append(loss_for_epoch)\n",
        "      opt.step() # this tells the optimizer to take one step -- it knows about model weights/biases and learning rate as we passed them when we create opt, this is where weigths are adjusted\n",
        "    return losses "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a lot going on in the fit function above. Try to read it over carefully and make sure you understand it. Some things to notice:\n",
        "* There is a loop over epochs -- we will pass over the data many times to train our NN\n",
        "* For each epoch, we pull batches of `xb,yb` from the training data and process each batch one at a time\n",
        "* Notice where `opt.step()` occurs, because this is where the weights/biases are adjusted\n"
      ],
      "metadata": {
        "id": "gTs8VcvKSgTl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNi-2x64diFW"
      },
      "source": [
        "## Fit the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLYzKPtOdkm4"
      },
      "source": [
        "Now we can train the model by running our fit function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBs31d7ndjrB"
      },
      "source": [
        "# Train the model for 100 epochs\n",
        "losses = fit_gd(100, model, train_dl, loss_fn, opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8n3WMSMdt4Z"
      },
      "source": [
        "# Get the final predictions (after we are done training)\n",
        "preds = model(inputs)\n",
        "\n",
        "# Plot predictions vs targets\n",
        "plt.plot(targets.detach().numpy(),preds.detach().numpy(),'.');\n",
        "plt.xlabel('targets');\n",
        "plt.ylabel('predictions');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU1XnCWSg2Er"
      },
      "source": [
        "One important thing that we DID NOT DO in the above two examples is that we didn't split the data into training and test sets.  We could have accomplished this using:\n",
        "```\n",
        "train_ds,test_ds = torch.utils.data.random_split(full_ds,[len_train, len_test])\n",
        "```\n",
        "\n",
        "But we didn't do this because this was just for illustration purposes and the dataset we are using is pretty small."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>\n",
        "<img src=\"https://drive.google.com/uc?id=1sk8CSP26YY7sfyzmHGFXncuNRujkvu9v\" align=\"left\">\n",
        "\n",
        "<font size=3 color=\"darkred\">Exercise:  Fit Function for Stochastic Gradient Descent  </font>\n",
        "\n",
        "The fit function provided above performs Gradient Descent. However, with just a few very small changes you can adapt this to be a fit function that uses Stochastic Gradient Descent (SGD). Write your code for an SGD fit function and call it `fit_sgd`:  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "73jClz1lW1bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it out"
      ],
      "metadata": {
        "id": "-bB9A-AcQqU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr/>"
      ],
      "metadata": {
        "id": "7bO5kk86Rudm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbI_xPeujHq-"
      },
      "source": [
        "# Saving and Loading pytorch models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpConHg7jL9K"
      },
      "source": [
        "You will want a way to save and (later to) load the models that you have trained. Pytorch has some tools to do this, using `torch.save()` and `torch.load()`.\n",
        "\n",
        "There are a few approaches:\n",
        "1. Save the entire model with `torch.save(model,path)` and then later load it using `model = torch.load(path)`\n",
        " - This will generate a save file that depends on the actual directory structure used in the specified path -- so you would only be able to load the save file if it was sitting in the same directory (even if you copied it to another system). So this way is not recommended. See the [pytorch docs](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for more info.\n",
        "2. Save the models state_dictionary only with `torch.save(model.state_dict(),path)`. When you want to load the model, you must then do the following:\n",
        "\n",
        "  ```python\n",
        "  model = Model() # make an instance of the model\n",
        "  model.load_state_dict(torch.load(path))\n",
        "  ```\n",
        "\n",
        "3. Save a checkpoint that captures the model's state_dictionary and other state variables, such as the optimizer's state_dictionary, current epoch, and so on. This can be done by passing your own dictionary to `torch.save()` like this:\n",
        " \n",
        " ```python \n",
        " torch.save({'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            ...\n",
        "            },\n",
        "            path)\n",
        " ```\n",
        " \n",
        " Loading the model is then accomplished like this:\n",
        " \n",
        " ```python\n",
        " model = Model() # make an instance of the model\n",
        " checkpoint = torch.load(path)\n",
        " model.load_state_dict(checkpoint['model_state_dict'])\n",
        " ...\n",
        " opt.load_state_dict(checkpoint['opt_state_dict'])\n",
        " ...\n",
        " epoch = checkpoint['epoch']\n",
        " ```\n",
        "\n",
        "The 3rd way is the most bulletproof approach and the most configurable (you can add anything you want to the dictionary that you save -- including some text description). Unfortunately there is no \"standard file extension\" that is conventionally used, though it is common give torch saves the `.pt` or `.pth` file extension.  \n",
        "\n",
        "The consequence of this is: As long as you are consistent, you won't have issues saving and loading. But you may have trouble loading models that were saved by others, so be aware of this.\n",
        "\n",
        "**One thing to note: colab won't save your files if your session disconnects, so you should download them after you save them.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedback\n",
        "What did you think about this notebook? What questions do you have? Were any parts confusing? Write your thoughts in the text box below.\n",
        "\n",
        "<font size =2> note: You can double click this text box in colab to edit it.</font>\n",
        "\n",
        "PUT YOUR THOUGHTS HERE"
      ],
      "metadata": {
        "id": "JOPdib-mkdbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit\n",
        "Don't forget to submit your notebook before class! Make sure you have saved your work (**Colab Menu: File-> Save**) and then download a pure python copy (**Colab Menu: File-> Download -> Download .py**) and a python notebook copy (**Colab Menu: File-> Download -> Download .ipynb**). You will upload both of these to the assignment on the canvas page.\n"
      ],
      "metadata": {
        "id": "-Od5aIDFnSYP"
      }
    }
  ]
}